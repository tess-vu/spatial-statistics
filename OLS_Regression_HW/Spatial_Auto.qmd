---
title: "Spatial_Auto"
format: html
editor: visual
---

# **1. Spatial Lag and Spatial Error Regression**

```{r}
#| eval: false
#| echo: true

library(sf)
library(dplyr)
library(tibble)
library(spdep)
library(spgwr)
library(tmap)
library(spatialreg)
library(whitestrap)
library(lmtest)
library(tseries)

options(scipen=999)

```

```{r}
#Read the Regression Data shp
regress_data <- st_read("data//RegressionData.shp")
```

```{r}
#Create variable PLUS1, defined as NBELPOV100+1
#We already have LNNBELPOV and LNMEDHVAL in the shapefile (1.a.iv, 1.a.v in hw instructions)
regress_data <- regress_data %>%
  mutate(
    PLUS1 = NBelPov100 + 1,
  )

#Before proceeding, let’s examine the distributions of our variables and see whether we need to create log transformations of any of them
par(oma=c(0,0,2,0)) 
par(mfrow=c(1,3)) 
hist(regress_data$MEDHHINC, breaks = 50)
hist(regress_data$MEDHVAL, breaks = 50)
hist(regress_data$PCTVACANT, breaks = 50)

#PCTVANCANT has a spike at 0, so not transformation for them. MEDHHINC and MEDHVAL will be transformed/they already exist in the regress_data gdf. 
```

```{r}
#Graph of LNMED(HH)INC, LNMEDHVAL, and PCTVACANT
par(oma=c(0,0,2,0)) 
par(mfrow=c(1,3)) 
hist(regress_data$LNMEDINC, breaks = 50)
hist(regress_data$LNMEDHVAL, breaks = 50)
hist(regress_data$PCTVACANT, breaks = 50)
#I am making an assumption that LNMEDINC in the log of MEDHHINC. To reiterate, the LNMEDINC and LNMEDVHAL are present when you load the regress_data shp. 
```

## 2. Create a Queen Weight Matrix

```{r}
#defining neighbors for each of the block groups in philly using queen neighbors
queen_neighbors <- poly2nb(regress_data, row.names = regress_data$POLY_ID)
summary(queen_neighbors)
```

```{r}
#It is useful to examine where block groups with non-average neighbor patterns are situated before we run spatial analyses. This is because these outlier block groups will affect our spatial analyses. We will look at the block groups where there is only 1 neighbor, and block groups where there are 27 neighbors. The block groups themselves will be colored in red, and their neighbors in green


#see which region has only one neighbor
smallestnbcard<-card(queen_neighbors) #extract neighbor matrix
smallestnb<-which(smallestnbcard == min(smallestnbcard)) #extract block groups with smallest number of neighbors
fg<-rep('grey90', length(smallestnbcard))
fg[smallestnb]<-'red' #color block groups red
fg[queen_neighbors[[smallestnb[1]]]]<-'green' #color neighboring blocks green
fg[queen_neighbors[[smallestnb[2]]]]<-'green'
fg[queen_neighbors[[smallestnb[3]]]]<-'green'
fg[queen_neighbors[[smallestnb[4]]]]<-'green'
plot(regress_data$geometry, col=fg)
title(main='Regions with only 1 neighbor')
```

```{r}
#block group with most neighbors 
largestnbcard<-card(queen_neighbors)
largestnb<-which(largestnbcard == max(largestnbcard))
fg1<-rep('grey90', length(largestnbcard))
fg1[largestnb]<-'red'
fg1[queen_neighbors[[largestnb]]]<-'green'
plot(regress_data$geometry, col=fg1)
title(main='Region with 27 neighbors')
```

## 3. Global Moran's I

```{r}
#Choosing W as the style parameter, which will perform row standardization.
queenlist <- nb2listw(queen_neighbors, style = "W")

moran(regress_data$LNMEDINC, queenlist, n=length(queenlist$neighbours), S0 =Szero(queenlist))$"I"
```

```{r}
# Check if this measure is statistically significant
moranI_mc <- moran.mc(regress_data$LNMEDINC, queenlist, nsim=999, alternative="two.sided")  #We use 999 permutations
moranI_mc

```
```{r}
# Histogram of the Moran’s I values from random permutations
moranI_mc_res<-moranI_mc$res
hist(moranI_mc_res, freq=10000000, nclass=100)   #Draws distribution of Moran's I's calculated from randomly permuted values
# Here, we draw a red vertical line at the observed value of our Moran's I
abline(v=moran(regress_data$LNMEDINC, queenlist, n=length(queenlist$neighbours), S0=Szero(queenlist))$`I`, col='red')  
```
```{r}
#Create Moran plot (lagged value against observed value)
moran.plot(regress_data$LNMEDINC, queenlist) 
```

## 4. Local Moran's I

The localmoran function allows us to compute LISA statistics for each block group. This function generates the Local Moran statistic as Ii. We might also be interested in Var.Ii, which tells us how much each block group’s Local Moran Statistic varies from the global mean, and Pr(z != E(Ii)), the p-value which indicates whether the Local Moran Statistic can be considered statistically significant.

```{r}
#Run local moran's I (LISA) 
lmoran<-localmoran(regress_data$LNMEDINC, queenlist)
head(lmoran)
```
```{r}
# Combine the original regression dataset with the Local Moran's I results so that each observation includes its corresponding local spatial statistic
lmoran_df <-cbind(regress_data, as.data.frame(lmoran))
```

We can create some maps using the tmap package. Specifically, we can create the p-value map and the cluster map, as in GeoDa. There’s a discussion of these and other maps available here: https://www.paulamoraga.com/book-spatial/spatial-autocorrelation.html.

```{r}
library(tmap)
tmap_mode("plot")

#Obtaining the Local Moran's P-Values (two-sided)
regress_data$local_I <- lmoran[, "Pr(z != E(Ii))"]
library(sf)
regress_data <- st_make_valid(regress_data) #Sometimes necessary if projection is off


#Creating the LISA Clusters
moran_plot <- moran.plot(as.vector(scale(regress_data$LNMEDINC)), queenlist)
```
```{r}
regress_data$quadrant <- NA
# high-high
regress_data[(moran_plot$x >= 0 & moran_plot$wx >= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 1
# low-low
regress_data[(moran_plot$x <= 0 & moran_plot$wx <= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 2
# high-low
regress_data[(moran_plot$x >= 0 & moran_plot$wx <= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 3
# low-high
regress_data[(moran_plot$x <= 0 & moran_plot$wx >= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 4
# non-significant
regress_data[(regress_data$local_I > 0.05), "quadrant"] <- 5
```


```{r}
# LISA P-Value Map
p_vals <- tm_shape(regress_data) +
  tm_polygons(col = "local_I", title = "",
              breaks = c(-Inf, 0.001, 0.01, 0.05, Inf),
              palette = c("darkblue", "blue", "lightblue", "white")) +
  tm_layout(
    legend.outside = TRUE,
    legend.text.size = 1,
    legend.title.size = 1,
    fontfamily = "Arial",
    title = "LISA P-Value Map",
    title.size = 1.2,
    frame = FALSE
  )

p_vals
```


```{r}
# LISA Cluster Map
lisa_clusters <- tm_shape(regress_data) +
  tm_fill(col = "quadrant", title = "",
          breaks = c(1, 2, 3, 4, 5, 6),
          palette = c("red", "blue", "lightpink", "skyblue2", "white"),
          labels = c("High-High", "Low-Low", "High-Low", "Low-High", "Non-significant")) +
  tm_borders(alpha = 0.5) +
  tm_layout(
    frame = FALSE,
    legend.outside = TRUE,
    legend.text.size = 1,
    legend.title.size = 1,
    fontfamily = "Arial",
    title = "LISA Cluster Map",
    title.size = 1.2
  )

lisa_clusters
```

## 5. Regression Analysis: OLS Regression
First, let’s run an OLS regression. The logLik command prints the log likelihood. We can also run the same tests for heteroscedasticity and normality of residuals that we see in the GeoDa output.

```{r}
# Fit an Ordinary Least Squares (OLS) regression model
# Dependent variable: LNMEDINC (log of median income)
# Independent variables: LNMEDHVAL (log of median home value) and PCTVACANT (percent of vacant housing)
ols_reg<-lm(formula=LNMEDINC ~ LNMEDHVAL + PCTVACANT, data=regress_data)

summary(ols_reg)
```
```{r}
#Prints the log likelihood
logLik(ols_reg)  
```
```{r}
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest(ols_reg, studentize=FALSE)
```
```{r}
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest(ols_reg)
```
```{r}
#Prints the results of the White Test to assess whether heteroscedasticity is present (package: whitestrap)
white_test(ols_reg)   
```
```{r}
# Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(ols_reg$residuals)
```
Now, let’s generate standardized residuals, which are OLS Model residuals divided by an estimate of their standard deviation, and map them. Visually, it certainly seems that there’s spatial autocorrelation in the residuals, with some higher values clustered in the northeast and northwest of the city, and some lower values clustered in north Philadelphia and downtown. However, a visual assessment is not sufficient, and we will test the presence of spatial autocorrelation in two ways: 1) by regressing residuals on their queen neighbors, and 2) by looking at the Moran’s I of the residuals.

```{r}
# Compute standardized residuals from the OLS regression which makes residuals comparable across observations
standardised_res<-rstandard(ols_reg)
# For each observation, calculate the mean standardized residual of its neighbors to identify whether high or low residuals cluster spatially
resnb<-sapply(queen_neighbors, function(x) mean(standardised_res[x]))

regress_data$standardised_res <- standardised_res    #creating a new variable in the shapefile shp.
OLS.Residuals.Map<-tm_shape(regress_data)+
  tm_fill(col='standardised_res', style='quantile', title='Standardized OLS Residuals', 
          palette ='Blues')+
  tm_layout(frame=FALSE, title = 'Standardised OLS Residuals')
OLS.Residuals.Map
```
First, let’s regress the OLS standardized residuals on the spatial lag of the OLS residuals (i.e., OLS residuals at the queen neighbors). We can see that the beta coefficient of the lagged residuals is significant and positive (0.598, p<0.0001), meaning that there’s a significant level of spatial autocorrelation in the residuals. This is consistent with Moran’s I of the residuals we see below.
```{r}
#Regressing residuals on their nearest neighbors.
regress_res_lm <- lm(formula=standardised_res ~ resnb)

summary(regress_res_lm)
```
Again, we can use moran.mc to generate a Moran’s I statistic and a pseudo p-value.
```{r}
# Perform a Monte Carlo test for Moran’s I on the standardized OLS residuals
# Arguments:
#  - standardised_res: residuals from the OLS model, standardized for comparability
#  - queenlist: the spatial weights list (queen contiguity)
#  - 999: number of random permutations (higher = more precise p-value)
#  - alternative="two.sided": tests both positive and negative spatial autocorrelation

moran.mc(standardised_res, queenlist, 999, alternative="two.sided")
```
```{r}
# The plot shows the relationship between each observation’s residual and the average residuals of its neighboring observations
# Arguments:
#  - standardised_res: residuals from the OLS model, standardized for comparability
#  - queenlist: the spatial weights list (queen contiguity)

moran.plot(standardised_res, queenlist)
```
From the above, it is strongly apparent that spatial autocorrelation exists among the regression residuals of the OLS Model. The p-value is very small indicating that Moran’s I is significant. Because there’s clearly spatial autocorrelation in OLS residuals, the OLS Model is inappropriate and we need to consider another method. Here, we will attempt to run the Spatial Lag Model, the Spatial Error Model, and Geographically Weighted Regression.

## 6. Regression Analysis: Spatial Lag Regression

To fit a Spatial Lag Model, we can use the lagsarlm function. This function works similarly to the lm function we are familiar with. The only key difference lies in the need to specify a weight matrix (neighbor list) created using nb2listw function.

From the results, we can see that the rho parameter (spatial lag of LNMEDHHINC), has a value of 0.44383 and is significant. We also get the AIC for the Spatial Lag Model (1207.8) which is substantially smaller than the AIC for the linear model (lm AIC = 1443.3). We also get the log likelihood (-598.91), which is higher (i.e., better) than the value we get in OLS using the loglik command (-717.6489). The LR.Sarlm command does a likelihood ratio test comparing the Spatial Lag Model to the OLS Model. The null hypothesis here is that the Spatial Lag Model isn’t better than OLS, which we can reject (p-value <0.00001).

```{r}
# Dependent variable: LNMEDINC (log of median income)
# Independent variables: LNMEDHVAL (log of median home value), PCTVACANT (percent vacant housing)
# Spatial weights: queenlist (based on queen contiguity)
lag_reg<-lagsarlm(formula=LNMEDINC ~ LNMEDHVAL + PCTVACANT, data=regress_data, queenlist)

summary(lag_reg)
```
```{r}
LR.Sarlm(lag_reg, ols_reg) #Here lagreg is the SL output; reg is the OLS output
```
```{r}
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(lag_reg, studentize=FALSE)
```
```{r}
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(lag_reg)  
```
```{r}
#Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(lag_reg$residuals)
```
Now, we can map the Spatial Lag Model residuals (which is an exercise left for the student) and look at the the Moran’s I of the Spatial Lag Model residuals. We see that these residuals from the Spatial Lag Model aren’t spatially autocorrelated, which is exactly what we were hoping to achieve.
```{r}
res_lag <- lag_reg$residuals

# This checks whether any spatial autocorrelation remains after accounting for the spatial lag
# Arguments:
#  - res_lag: residuals from the spatial lag model
#  - queenlist: spatial weights list (queen contiguity)
#  - 999: number of random permutations for the Monte Carlo simulation
#  - alternative="two.sided": tests both positive and negative spatial autocorrelation
lag_moran_mc <- moran.mc(res_lag, queenlist,999, alternative="two.sided")

lag_moran_mc
```

## 7. Regression Analysis: Spatial Error Regression

To fit a Spatial Error Model, we use the errorsarlm function.

Here, we see that lambda has the value of 0.45391 and is significant. We can also look at the AIC and the log likelihood. The AIC here is 1271.8, which is lower (better) than in OLS but higher (worse) than in the Spatial Lag Model. The log likelihood here is -630.899, which is higher than the value we see for OLS. The LR.Sarlm command does a likelihood ratio test comparing the Spatial Error Model to the OLS Model. The null hypothesis here is that the Spatial Error Model isn’t better than OLS, which we can reject (p-value <0.00001).

```{r}
error_reg <- errorsarlm(formula=LNMEDINC ~ LNMEDHVAL + PCTVACANT, data=regress_data, queenlist)
# Extract residuals from the fitted spatial error model
error_res <- residuals(error_reg)
# Compute the mean residual value for each observation's neighbors
error_res_mean <- sapply(queen_neighbors, function(x) mean(error_res[x]))

summary(error_reg)
```


## 8. Spatial Lag Regression (w/ Moran's I scatterplot of 99 permutations)

## 9. Spatial Error Regression (w/ Moran's I scatterplot of 99 permutations)

### 9a. Compare

## 10. Geographically Weighted Regression (GWR)

### 10a. Global Regression Output (global R\^2, AIC, AICc)

### 10b. Choropeth map of local R\^2 values

### 10c. Moran's I scatterplot and random permutations test for GWR residuals

### 10d?. Follow slide instructions to map local regression results
