---
title: "Spatial_Auto"
format: html
editor: visual
---

# **1. Spatial Lag and Spatial Error Regression**

```{r}
#| eval: false
#| echo: true

library(sf)
library(dplyr)
library(tibble)
library(spdep)
library(spgwr)
library(tmap)
library(spatialreg)
library(whitestrap)
library(lmtest)
library(tseries)

options(scipen=999)

```

```{r}
#Read the Regression Data shp
regress_data <- st_read("data/RegressionData.shp")
```

```{r}
#Create variable PLUS1, defined as NBELPOV100+1
#We already have LNNBELPOV and LNMEDHVAL in the shapefile (1.a.iv, 1.a.v in hw instructions)
regress_data <- regress_data %>%
  mutate(
    PLUS1 = NBelPov100 + 1,
  )

#Before proceeding, let’s examine the distributions of our variables and see whether we need to create log transformations of any of them
par(oma=c(0,0,2,0)) 
par(mfrow=c(1,3)) 
hist(regress_data$MEDHHINC, breaks = 50)
hist(regress_data$MEDHVAL, breaks = 50)
hist(regress_data$PCTVACANT, breaks = 50)

#PCTVANCANT has a spike at 0, so not transformation for them. MEDHHINC and MEDHVAL will be transformed/they already exist in the regress_data gdf. 
```

```{r}
#Graph of LNMED(HH)INC, LNMEDHVAL, and PCTVACANT
par(oma=c(0,0,2,0)) 
par(mfrow=c(1,3)) 
hist(regress_data$LNMEDINC, breaks = 50)
hist(regress_data$LNMEDHVAL, breaks = 50)
hist(regress_data$PCTVACANT, breaks = 50)
#I am making an assumption that LNMEDINC in the log of MEDHHINC. To reiterate, the LNMEDINC and LNMEDVHAL are present when you load the regress_data shp. 
```

## 2. Create a Queen Weight Matrix

```{r}
#defining neighbors for each of the block groups in philly using queen neighbors
queen_neighbors <- poly2nb(regress_data, row.names = regress_data$POLY_ID)
summary(queen_neighbors)
```

```{r}
#It is useful to examine where block groups with non-average neighbor patterns are situated before we run spatial analyses. This is because these outlier block groups will affect our spatial analyses. We will look at the block groups where there is only 1 neighbor, and block groups where there are 27 neighbors. The block groups themselves will be colored in red, and their neighbors in green


#see which region has only one neighbor
smallestnbcard<-card(queen_neighbors) #extract neighbor matrix
smallestnb<-which(smallestnbcard == min(smallestnbcard)) #extract block groups with smallest number of neighbors
fg<-rep('grey90', length(smallestnbcard))
fg[smallestnb]<-'red' #color block groups red
fg[queen_neighbors[[smallestnb[1]]]]<-'green' #color neighboring blocks green
fg[queen_neighbors[[smallestnb[2]]]]<-'green'
fg[queen_neighbors[[smallestnb[3]]]]<-'green'
fg[queen_neighbors[[smallestnb[4]]]]<-'green'
plot(regress_data$geometry, col=fg)
title(main='Regions with only 1 neighbor')
```

```{r}
#block group with most neighbors 
largestnbcard<-card(queen_neighbors)
largestnb<-which(largestnbcard == max(largestnbcard))
fg1<-rep('grey90', length(largestnbcard))
fg1[largestnb]<-'red'
fg1[queen_neighbors[[largestnb]]]<-'green'
plot(regress_data$geometry, col=fg1)
title(main='Region with 27 neighbors')
```

## 3. Global Moran's I

```{r}
#Choosing W as the style parameter, which will perform row standardization.
queenlist <- nb2listw(queen_neighbors, style = "W")

moran(regress_data$LNMEDHVAL, queenlist, n=length(queenlist$neighbours), S0 =Szero(queenlist))$"I"
```

```{r}
# Check if this measure is statistically significant
moranI_mc <- moran.mc(regress_data$LNMEDHVAL, queenlist, nsim=999, alternative="two.sided")  #We use 999 permutations
moranI_mc

```

```{r}
# Histogram of the Moran’s I values from random permutations
moranI_mc_res<-moranI_mc$res
hist(moranI_mc_res, freq=10000000, nclass=100)   #Draws distribution of Moran's I's calculated from randomly permuted values
# Here, we draw a red vertical line at the observed value of our Moran's I
abline(v=moran(regress_data$LNMEDHVAL, queenlist, n=length(queenlist$neighbours), S0=Szero(queenlist))$`I`, col='red')  
```

**Monte Carlo Interpretation:** 0.55247 Moran's I value indicates strong positive spatial autocorrelation, so many of the block groups are clustered together with high-income areas aside other high-income areas and low-income areas aside other low-income areas. The histogram shows that the spatial pattern observed in Philadelphia's block groups (red line) are leagues beyond, and statistically significant, compared to the random permutations in the histogram. That is, there's a less than 1% chance that Philadelphia's positive spatial autocorrelation occurred by random chance.

```{r}
#Create Moran plot (lagged value against observed value)
moran.plot(regress_data$LNMEDHVAL, queenlist) 
```

**Moran's I Plot Interpretation:** Many of the observations seem to fit along the positive line running through the lower left low-low quadrant and the upper right high-high quadrant, which also indicates positive spatial autocorrelation.

## 4. Local Moran's I

The localmoran function allows us to compute LISA statistics for each block group. This function generates the Local Moran statistic as Ii. We might also be interested in Var.Ii, which tells us how much each block group’s Local Moran Statistic varies from the global mean, and Pr(z != E(Ii)), the p-value which indicates whether the Local Moran Statistic can be considered statistically significant.

```{r}
#Run local moran's I (LISA) 
lmoran<-localmoran(regress_data$LNMEDHVAL, queenlist)
head(lmoran)
```

```{r}
# Combine the original regression dataset with the Local Moran's I results so that each observation includes its corresponding local spatial statistic
lmoran_df <-cbind(regress_data, as.data.frame(lmoran))
```

We can create some maps using the tmap package. Specifically, we can create the p-value map and the cluster map, as in GeoDa. There’s a discussion of these and other maps available here: https://www.paulamoraga.com/book-spatial/spatial-autocorrelation.html.

```{r}
library(tmap)
tmap_mode("plot")

#Obtaining the Local Moran's P-Values (two-sided)
regress_data$local_I <- lmoran[, "Pr(z != E(Ii))"]
library(sf)
regress_data <- st_make_valid(regress_data) #Sometimes necessary if projection is off


#Creating the LISA Clusters
moran_plot <- moran.plot(as.vector(scale(regress_data$LNMEDHVAL)), queenlist)
```

```{r}
regress_data$quadrant <- NA
# high-high
regress_data[(moran_plot$x >= 0 & moran_plot$wx >= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 1
# low-low
regress_data[(moran_plot$x <= 0 & moran_plot$wx <= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 2
# high-low
regress_data[(moran_plot$x >= 0 & moran_plot$wx <= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 3
# low-high
regress_data[(moran_plot$x <= 0 & moran_plot$wx >= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 4
# non-significant
regress_data[(regress_data$local_I > 0.05), "quadrant"] <- 5
```

```{r}
# LISA P-Value Map
p_vals <- tm_shape(regress_data) +
  tm_polygons(col = "local_I", title = "",
              breaks = c(-Inf, 0.001, 0.01, 0.05, Inf),
              palette = c("darkblue", "blue", "lightblue", "white")) +
  tm_borders(col = "gray", lwd = 0.5) +
  tm_layout(
    legend.outside = TRUE,
    legend.text.size = 1,
    legend.title.size = 1,
    fontfamily = "Arial",
    title = "LISA P-Value Map",
    title.size = 1.2,
    frame = FALSE
  )

p_vals

#tmap_save(p_vals, filename = "HW2_Plots/p_vals_Map.png", width = 8, height = 8, dpi = 300)
```

**LISA p-Value Map Interpretation:** Many statistically significant block clusters deeper in northeast and northwest Philadelphia, as well as parts of West Philadelphia in the Mantua neighborhood above Market St, North Philadelphia in the Strawberry Mansion and Kensington neighborhoods, and very east bordering along the Delaware River. Whether these are high-high or low-low priced areas is not indicated on this map, but the map below.

```{r}
# LISA Cluster Map
lisa_clusters <- tm_shape(regress_data) +
  tm_fill(col = "quadrant", title = "",
          breaks = c(1, 2, 3, 4, 5, 6),
          palette = c("red", "blue", "lightpink", "skyblue2", "white"),
          labels = c("High-High", "Low-Low", "High-Low", "Low-High", "Non-significant")) +
  tm_borders(alpha = 0.5) +
  tm_borders(col = "gray", lwd = 0.5) +
  tm_layout(
    frame = FALSE,
    legend.outside = TRUE,
    legend.text.size = 1,
    legend.title.size = 1,
    fontfamily = "Arial",
    title = "LISA Cluster Map",
    title.size = 1.2
  )

lisa_clusters

#tmap_save(lisa_clusters, filename = "HW2_Plots/lisa_clusters_Map.png", width = 8, height = 8, dpi = 300)
```

**LISA Cluster Map:** Aligning with the p-Value map above, it looks like those clustered spaces in Northeast and Northwest Philadelphia as well as along the Delaware River are high-high clusters compared to the rest of the block groups, where the North Philadelphia and West Philadelphia areas are low-low clusters.

## 5. Regression Analysis: OLS Regression

First, let’s run an OLS regression. The logLik command prints the log likelihood. We can also run the same tests for heteroscedasticity and normality of residuals that we see in the GeoDa output.

```{r}
# Fit an Ordinary Least Squares (OLS) regression model
# Dependent variable: LNMEDINC (log of median income)
# Independent variables: LNMEDHVAL (log of median home value) and PCTVACANT (percent of vacant housing)
ols_reg<-lm(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT, data=regress_data)

summary(ols_reg)
```

**Regression Interpretation:** We are predicting LNMEDINC (log of median income), the \[y-hat_i\], with predictors LNMEDHVAL (log of median house value) and PCTVACANT (percent vacancies), respectively \[x_1i\] and \[x_2i\].

The expected percent of median income when all predictors are 0 is 5.650956%. However, this is an unrealistic baseline estimate. When it comes to LNMEDHVAL, the output indicates that a 1% increase in median home value is associated with a 0.421546% increase in median income, holding percent vacancies constant. With PCTVACANT, the output indicates that a 1% increase in vacancies is associated with a 0.009191% decrease in median income, holding median home value constant. Both predictors show p \< 0.001, so their very low p-values indicate high statistic significance.

The residuals, or the error between the predicted value and the observed value, look to be centered very near 0, so there's seemingly no bias when looking at just the numbers. However, residual plots to visualize any heteroscedasticity will need to be plotted. The residuals seem to deviate from -2.53728 to 1.26485, which signals somewhat more underestimation for this model than overestimation.

This model's adjusted R-squared at 0.4403 indicates it explains around 44.03% of the variation in the model, a very marginal adjustment from the original R-squared at 0.441, or 44.10%.

The F-statistic value of 677.3 indicates high statistical significance, meaning that at least one predictor is useful.

The residual standard error 0.3676 indicates that the model is, on average, off on its predictions by 0.3676.

```{r}
# OLS Residual vs. WT Residual

regress_data$OLS_RESIDU <- residuals(ols_reg)

regress_data$WT_RESIDU <- lag.listw(queenlist, regress_data$OLS_RESIDU)

moran_RESIDU <- moran.plot(regress_data$OLS_RESIDU, queenlist)
```

```{r}
#png("OLS_RESIDU_Plot.png", width = 8, height = 6, units = "in", res = 150)
#moran.plot(regress_data$OLS_RESIDU, queenlist)
#dev.off()
```

```{r}
names(moran_RESIDU)

summary(lm(moran_RESIDU$wx ~ moran_RESIDU$x))
```

```{r}
#Prints the log likelihood
logLik(ols_reg)  
```

**Log Likelihood Interpretation:** The number of parameters, 4, indicate the response and two predictor variables along with the residual variance. This measures how well the predicted values match the observed data under assumed error distribution. A higher, or less negative value means a better fit for the data, whereas a lower, or more negative value, means a worse fit for the data. However, value is used as a comparison, so observing in isolation is not generally helpful.

```{r}
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest(ols_reg, studentize=FALSE)
```

**Breusch-Pagan Test Interpretation:** This is used to determine heteroscedasticity in a regression model, akin to plotting residuals. The BP is 68.649 and p-value is significantly less than 0.001, so this means the we reject the null hypothesis, indicating that the data is heteroscedastic. Of course, this violates one of the OLS assumptions for normality of residuals.

Spatially speaking, the variance of the errors not being constant across all observations means that the prediction errors are overestimated or underestimated for some blocks as opposed to others.

```{r}
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest(ols_reg)
```

**Studentized Breusch-Pagan Test Interpretation:** Like the previous test, it observes for any heteroscedasticity, the only difference being that the original assumes normality, whereas this modification doesn't. The Studentized version is less stringent and adjusts for variance, this makes it better to use when certain data is known to be highly skewed, like this paper's analysis of median income with housing prices and vacancies in Philadelphia. Again, with this output, the CP is 20.892 with a very small p-value, meaning reject the null hypothesis, and the data is heteroscedastic.

```{r}
#Prints the results of the White Test to assess whether heteroscedasticity is present (package: whitestrap)
white_test(ols_reg)   
```

**White's Test Interpretation:** This homoscedasticity test's main differentiator from Breusch-Pagan is that it can test for non-linear heteroscedasticity. While this is a very robust test, it doesn't have as strong influence when sample sizes are small.

```{r}
# Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(ols_reg$residuals)
```

**Jarque Bera Test Interpretation:** This test observes for normal residuals, with the null hypothesis being that the residuals are normally distributed and the alternative hypothesis that the residuals are not normally distributed. The p-value is also very small, meaning we reject the null hypothesis. Because there are so many block groups, this does not bias coefficients, but combined with heteroscedasticity, the p-values may be slightly off.

Now, let’s generate standardized residuals, which are OLS Model residuals divided by an estimate of their standard deviation, and map them. Visually, it certainly seems that there’s spatial autocorrelation in the residuals, with some higher values clustered in the northeast and northwest of the city, and some lower values clustered in north Philadelphia and downtown. However, a visual assessment is not sufficient, and we will test the presence of spatial autocorrelation in two ways: 1) by regressing residuals on their queen neighbors, and 2) by looking at the Moran’s I of the residuals.

```{r}
# Compute standardized residuals from the OLS regression which makes residuals comparable across observations
standardised_res<-rstandard(ols_reg)
# For each observation, calculate the mean standardized residual of its neighbors to identify whether high or low residuals cluster spatially
resnb<-sapply(queen_neighbors, function(x) mean(standardised_res[x]))

regress_data$standardised_res <- standardised_res    #creating a new variable in the shapefile shp.
```

```{r}
OLS.Residuals.Map <- tm_shape(regress_data)+
  tm_fill(col='standardised_res', 
          style='quantile', 
          title='Standardized OLS Residuals', 
          palette ='Blues')+
  tm_borders(col = "gray", lwd = 0.5) +  # Add this line
  tm_layout(frame=FALSE, title='Standardized OLS Residuals')

OLS.Residuals.Map

#tmap_save(OLS.Residuals.Map, filename = "HW2_Plots/OLS_Residuals_Map.png", width = 8, height = 8, dpi = 300)
```

First, let’s regress the OLS standardized residuals on the spatial lag of the OLS residuals (i.e., OLS residuals at the queen neighbors). We can see that the beta coefficient of the lagged residuals is significant and positive (0.598, p\<0.0001), meaning that there’s a significant level of spatial autocorrelation in the residuals. This is consistent with Moran’s I of the residuals we see below.

```{r}
#Regressing residuals on their nearest neighbors.
regress_res_lm <- lm(formula=standardised_res ~ resnb)

summary(regress_res_lm)
```

Again, we can use moran.mc to generate a Moran’s I statistic and a pseudo p-value.

```{r}
# Perform a Monte Carlo test for Moran’s I on the standardized OLS residuals
# Arguments:
#  - standardised_res: residuals from the OLS model, standardized for comparability
#  - queenlist: the spatial weights list (queen contiguity)
#  - 999: number of random permutations (higher = more precise p-value)
#  - alternative="two.sided": tests both positive and negative spatial autocorrelation

moran.mc(standardised_res, queenlist, 999, alternative="two.sided")
```

```{r}
# The plot shows the relationship between each observation’s residual and the average residuals of its neighboring observations
# Arguments:
#  - standardised_res: residuals from the OLS model, standardized for comparability
#  - queenlist: the spatial weights list (queen contiguity)

moran.plot(standardised_res, queenlist)
```

From the above, it is strongly apparent that spatial autocorrelation exists among the regression residuals of the OLS Model. The p-value is very small indicating that Moran’s I is significant. Because there’s clearly spatial autocorrelation in OLS residuals, the OLS Model is inappropriate and we need to consider another method. Here, we will attempt to run the Spatial Lag Model, the Spatial Error Model, and Geographically Weighted Regression.

## 6. Regression Analysis: Spatial Lag Regression

To fit a Spatial Lag Model, we can use the lagsarlm function. This function works similarly to the lm function we are familiar with. The only key difference lies in the need to specify a weight matrix (neighbor list) created using nb2listw function.

From the results, we can see that the rho parameter (spatial lag of LNMEDHHINC), has a value of 0.44383 and is significant. We also get the AIC for the Spatial Lag Model (1207.8) which is substantially smaller than the AIC for the linear model (lm AIC = 1443.3). We also get the log likelihood (-598.91), which is higher (i.e., better) than the value we get in OLS using the loglik command (-717.6489). The LR.Sarlm command does a likelihood ratio test comparing the Spatial Lag Model to the OLS Model. The null hypothesis here is that the Spatial Lag Model isn’t better than OLS, which we can reject (p-value \<0.00001).

```{r}
# Dependent variable: LNMEDINC (log of median income)
# Independent variables: LNMEDHVAL (log of median home value), PCTVACANT (percent vacant housing)
# Spatial weights: queenlist (based on queen contiguity)
lag_reg<-lagsarlm(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT, data=regress_data, queenlist)

summary(lag_reg)
```

```{r}
LR.Sarlm(lag_reg, ols_reg) #Here lag_reg is the SL output; ols_reg is the OLS output
```

```{r}
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(lag_reg, studentize=FALSE)
```

```{r}
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(lag_reg)  
```

```{r}
#Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(lag_reg$residuals)
```

Now, we can map the Spatial Lag Model residuals (which is an exercise left for the student) and look at the the Moran’s I of the Spatial Lag Model residuals. We see that these residuals from the Spatial Lag Model aren’t spatially autocorrelated, which is exactly what we were hoping to achieve.

```{r}
lag_res <- lag_reg$residuals

# This checks whether any spatial autocorrelation remains after accounting for the spatial lag
# Arguments:
#  - res_lag: residuals from the spatial lag model
#  - queenlist: spatial weights list (queen contiguity)
#  - 999: number of random permutations for the Monte Carlo simulation
#  - alternative="two.sided": tests both positive and negative spatial autocorrelation
lag_moran_mc <- moran.mc(lag_res, queenlist,999, alternative="two.sided")

lag_moran_mc
```

```{r}
# Lag residual plot
moran.plot(lag_res, queenlist)
```

## 7. Regression Analysis: Spatial Error Regression

To fit a Spatial Error Model, we use the errorsarlm function.

Here, we see that lambda has the value of 0.45391 and is significant. We can also look at the AIC and the log likelihood. The AIC here is 1271.8, which is lower (better) than in OLS but higher (worse) than in the Spatial Lag Model. The log likelihood here is -630.899, which is higher than the value we see for OLS. The LR.Sarlm command does a likelihood ratio test comparing the Spatial Error Model to the OLS Model. The null hypothesis here is that the Spatial Error Model isn’t better than OLS, which we can reject (p-value \<0.00001).

```{r}
error_reg <- errorsarlm(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT, data=regress_data, queenlist)
# Extract residuals from the fitted spatial error model
error_res <- residuals(error_reg)
# Compute the mean residual value for each observation's neighbors
error_res_mean <- sapply(queen_neighbors, function(x) mean(error_res[x]))

summary(error_reg)
```

```{r}
# Perform a likelihood ratio (LR) test comparing the spatial error model to OLS
#  - error_reg: fitted spatial error model (errorsarlm)
#  - ols_reg: fitted OLS model
LR.Sarlm(error_reg, ols_reg)
```

```{r}
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(error_reg, studentize=FALSE)
```

```{r}
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(error_reg)   
```

```{r}
#Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(error_reg$residuals)
```

```{r}
# Perform a Monte Carlo test for Moran's I on the residuals of the spatial error model
#  - error_res: residuals from the spatial error model (errorsarlm)
#  - queenlist: spatial weights list (queen contiguity)
#  - 999: number of random permutations for the Monte Carlo simulation
#  - alternative="two.sided": tests for both positive and negative spatial autocorrelation
error_moran_mc<-moran.mc(error_res, queenlist, 999, alternative="two.sided")

error_moran_mc
```

```{r}
# Error residual plot
moran.plot(error_res, queenlist)
```

## 8. Regression Analysis: Geographically Weighted Regression

###Bandwidth Selection

Unlike the OLS, SL and SE Models, GWR allows for spatial non-stationarity. It is based on the premise that the relationships between variables aren’t the same at all spatial locations.

Although for the previous analyses, we define neighbors, for the GWR analyses, we will define bandwidths. The bandwidth can be manually entered by the user, or it can be determined by R through cross-validation. Here, we will be using the spgwr package in R to carry out the bandwidth selection and run GWR.

We will use the gwr.sel function to calculate the optimal bandwidth for GWR. Let’s examine the parameters specified in the code below.

formula: We specify the OLS regression model formula we used in our OLS Model above. This is because a separate OLS equation for every location in the dataset in a GWR Model, and these separate OLS equations incorporate the dependent and explanatory variables of locations falling within the bandwidth of each target location.

data: You can specify a dataframe here, or a SpatialPointsDataFrame or SpatialPolgonsDataframe object that was defined in the package sp.

coords: If you specified a non-spatial dataframe above, it is neccessary to include a matrix of coordinates of points or polygons that represents the spatial positions of the observations. However, specification for this parameter is not neccessary if you specified a spatial dataframe, as was done for this example. Here, we specified a SpatialPointsDataFrame.

method: Here, you specify how an ‘optimal’ bandwidth should be defined. AIC is the approach used here to select a bandwidth that optimises the model AICc (corrected AIC), which is a relative measure of goodness of model fit. Another method you can specify here is cv, which selects a bandwidth that allows for the GWR results to be approximately equal acoss cross-validated folds - here, your objective is a generalisable GWR Model.

adapt: TRUE returns a proportion between 0 and 1 of the observations to include in the weighting scheme for an adaptive bandwidth. On the other hand, if a global fixed bandwidth that remains constant for all locations suit your purpose, set this parameter as FALSE.

Let’s start with adaptive bandwidth, which varies the distance, but fixes the number of neighbors for each observation. Given that we specified a preference for an adaptive bandwidth optimized based on AICc, the output (despite its labels) is not a distance, but rather, the proportion of observations to be included for each location in the GWR Model. Here, it is recommended that 0.007862703 (or \~0.786%) of the observations be used for each location - the bandwidth should be adapted to capture about 13 observations (0.00786\*1720) for each location. You might think this is too few observations to be fitted for each local regression - in this case, the printed AICc measures can serve as a guidance for the proportion you may input in the GWR Model later on. Keep in mind that finding the optimal value of bandwidth takes a while, so be patient while the code runs.

```{r}
# convert regress_data from df to spatial data
regress_spatial <- as(regress_data, 'Spatial')  #These analyses are easier to do when the data are of the SpatialPolygonsDataFrame class
class (regress_spatial)
```

```{r}
# Select an adaptive bandwidth for Geographically Weighted Regression (GWR)
#   - formula: dependent variable (LNMEDINC) and predictors (LNMEDHVAL, PCTVACANT)
#   - data: regress_spatial must be a SpatialPolygonsDataFrame (converted from sf)
#   - method = "aic": uses Akaike Information Criterion to select the optimal bandwidth
#   - adapt = TRUE: uses an adaptive bandwidth (proportion of nearest neighbors)
#     instead of a fixed distance bandwidth
bandwidth <- gwr.sel(formula = LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT, 
              data = regress_spatial,
              method = "aic",
              adapt = TRUE)
```

```{r}
bandwidth
```

We might also want to create a fixed bandwidth. Notice that the output values more closely approximate distance (in the units of the shapfile, i.e., feet) instead of a proportion between 0 and 1.

```{r}
#setting a fixed bandwidth
bandwidth_fixed<-gwr.sel(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT, 
            data=regress_spatial,
            method = "aic",
            adapt = FALSE)
```

```{r}
bandwidth_fixed
```

Running GWR To fit a GWR Model, we use the command gwr. Again, specify the formula as used in the previous OLS Model and the bandwidth defined in the earlier step.

We can also set a geographical weighting function for the bandwidth to specify how observations of varying distances from the location should be accounted for. If gweight=gwr.Gauss, the weights allocated to distributions vary normally like in a Gaussian (i.e., normal) distribution. If gwr.bisquare is specified, observations within the a certain distance threshold (specified by the bandwidth) from the location are weighted as 1, while observations beyond this threshold are weighted as 0.

We can compare the outputs from using an adaptive bandwidth specified by the proportion of observations included in each local regression (adapt=bw), and a fixed bandwidth (bandwidth=bw_fixed).

Below are the results using the adaptive bandwidth.

```{r}
# Formula: dependent variable (LNMEDINC) and predictors (LNMEDHVAL, PCTVACANT)
#   - data: regress_data (SpatialPolygonsDataFrame)
#   - adapt = bandwidth: uses the adaptive bandwidth selected earlier (proportion of nearest neighbors)
#   - gweight = gwr.Gauss: applies a Gaussian weighting kernel to give nearer observations more influence
#   - se.fit = TRUE: returns local standard errors for each coefficient estimate
#   - hatmatrix = TRUE: stores diagnostic info for model evaluation (e.g., local R²)
gwr_model<-gwr(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT,
              data=regress_spatial,
              adapt = bandwidth, #adaptive bandwidth determined by proportion of observations accounted for
              gweight=gwr.Gauss,
              se.fit=TRUE, #to return local standard errors
              hatmatrix = TRUE)

gwr_model
```

```{r}
# GWR adaptive moran's i
gwr_adaptive_residuals <- gwr_model$SDF$gwr.e

gwr_moran_mc <- moran.mc(gwr_adaptive_residuals, queenlist, nsim = 999)
gwr_moran_mc
```

```{r}
# GWR adaptive moran's i plot
gwr_adaptive_plot <- moran.plot(gwr_adaptive_residuals, queenlist)
```

Below are the results using the fixed bandwidth.

```{r}
# results using fixed bandwidth
gwr_model_fixed<-gwr(formula=LNMEDHVAL ~ LNNBELPOV + PCTBACHMOR + PCTSINGLES + PCTVACANT,
              data=regress_spatial,
              bandwidth = bandwidth_fixed, #fixed bandwidth
              gweight=gwr.Gauss,
              se.fit=TRUE, #to return local standard errors
              hatmatrix = TRUE)

gwr_model_fixed
```

```{r}
# GWR adaptive moran's i
gwr_fixed_residuals <- gwr_model_fixed$SDF$gwr.e

gwr_moran_mc <- moran.mc(gwr_fixed_residuals, queenlist, nsim = 999)
gwr_moran_mc
```

```{r}
# GWR adaptive moran's i plot
gwr_adaptive_plot <- moran.plot(gwr_fixed_residuals, queenlist)
```

Notice that when we construct our GWR Model, the output from gwr.sel (where adapt=TRUE) should be specified for the parameter adapt. On the other hand, if we were interested in a fixed bandwidth, the output from gwr.sel (where adapt=FALSE) should be specified for the parameter bandwidth. Also notice the difference in the results. We seem to be getting a better fit (based on AIC), less error (based on Residual sum of squares), and a slightly better global R2 using the GWR Model with an adaptive bandwidth. Keep in mind that when comparing the GWR Model to OLS, Spatial Lag and Spatial Error, you should use the AIC and not the AICc.

Presenting GWR Output Using Adaptive Bandwidth We can look at a summary of the coefficients of the local regressions, stored in the SDF object within thegwrmodel. Note in particular the minimum and maximum values of the Local R2 (0.05789 - 0.75169). There are no negative values, meaning that this output, unlike the output we get in some versions of ArcGIS Pro, is correct.

```{r}
# Summarize the SpatialDataFrame output from the GWR model
summary(gwr_model$SDF)
```

We can also map the standardized coefficients. The higher the absolute value of the ratio between the coefficient and the standard error, the more plausible it is that the relationship between the predictor and the dependent variable is significant at the location.

```{r}
gwr_results <- as.data.frame(gwr_model$SDF)

# LNNBELPOV
regress_data$coefLNNBELPOVst <- gwr_results$LNNBELPOV/gwr_results$LNNBELPOV_se

# PCTBACHMOR
regress_data$coefPCTBACHMORst <- gwr_results$PCTBACHMOR/gwr_results$PCTBACHMOR_se

# PCTSINGLES
regress_data$coefPCTSINGLESst <- gwr_results$PCTSINGLES/gwr_results$PCTSINGLES_se

# PCTVACANT
regress_data$coefPCTVACANTst <- gwr_results$PCTVACANT/gwr_results$PCTVACANT_se

regress_data$gwrE <- gwr_results$gwr.e
regress_data$localR2 <- gwr_results$localR2
```

```{r}
gwr_results
```

```{r}
#The maps above are the ones using the exact code that our professor had but it looked funky so here is the code that I made to try and fix it.

library(tmap)
```

```{r}
# LNNBELPOV
coefLNNBELPOV <- tm_shape(regress_data) +
  tm_fill(
    col = "coefLNNBELPOVst",
    breaks = c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf),
    title = "Standardized Coef: LNNBELPOV",
    palette = "-RdBu"
  ) +
  tm_borders(col = "gray", lwd = 0.5) +
  tm_layout(
    frame = FALSE,
    #title = "Percent of Number in Poverty (Log)",
    title.position = c("center", "top"),
    title.size = 1.2,
    legend.position = c("RIGHT", "BOTTOM"),
    legend.just = c("right", "bottom"),
    legend.bg.color = "white",
    legend.bg.alpha = 0.9,
    legend.text.size = 0.6,
    legend.title.size = 0.6,
    inner.margins = c(0.1, 0.15, 0.1, 0.15)
  )

coefLNNBELPOV

#tmap_save(coefLNNBELPOV, filename = "HW2_Plots/coefLNNBELPOV_Map.png", width = 8, height = 8, dpi = 300)
```

```{r}
# PCTBACHMOR
coefPCTBACHMOR <- tm_shape(regress_data) +
  tm_fill(
    col = "coefPCTBACHMORst",
    breaks = c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf),
    title = "Standardized Coef: PCTBACHMOR",
    palette = "-RdBu"
  ) +
  tm_borders(col = "gray", lwd = 0.5) +
  tm_layout(
    frame = FALSE,
    #title = "Percent of Bachelor's or More",
    title.position = c("center", "top"),
    title.size = 1.2,
    legend.position = c("RIGHT", "BOTTOM"),
    legend.just = c("right", "bottom"),
    legend.bg.color = "white",
    legend.bg.alpha = 0.9,
    legend.text.size = 0.6,
    legend.title.size = 0.6,
    inner.margins = c(0.1, 0.15, 0.1, 0.15)
  )

coefPCTBACHMOR

#tmap_save(coefPCTBACHMOR, filename = "HW2_Plots/coefPCTBACHMOR_Map.png", width = 8, height = 8, dpi = 300)
```

```{r}
coefPCTSINGLES <- tm_shape(regress_data) +
  tm_fill(
    col = "coefPCTSINGLESst",
    breaks = c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf),
    title = "Standardized Coef: PCTSINGLES",
    palette = "-RdBu"
  ) +
  tm_borders(col = "gray", lwd = 0.5) +
  tm_layout(
    frame = FALSE,
    #title = "Percent of Detached Single Homes",
    title.position = c("center", "top"),
    title.size = 1.2,
    legend.position = c("RIGHT", "BOTTOM"),
    legend.just = c("right", "bottom"),
    legend.bg.color = "white",
    legend.bg.alpha = 0.9,
    legend.text.size = 0.6,
    legend.title.size = 0.6,
    inner.margins = c(0.1, 0.15, 0.1, 0.15)
  )

coefPCTSINGLES

#tmap_save(coefPCTSINGLES, filename = "HW2_Plots/coefPCTSINGLES_Map.png", width = 8, height = 8, dpi = 300)
```

```{r}
# PCTVACANT
coefPCTVACANT <- tm_shape(regress_data) +
  tm_fill(
    col = "coefPCTVACANTst",
    breaks = c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf),
    title = "Standardized Coef: PCTVACANT",
    palette = "-RdBu"
  ) +
  tm_borders(col = "gray", lwd = 0.5) +
  tm_layout(
    frame = FALSE,
    #title = "Percent of Vacancies",
    title.position = c("center", "top"),
    title.size = 1.2,
    legend.position = c("RIGHT", "BOTTOM"),
    legend.just = c("right", "bottom"),
    legend.bg.color = "white",
    legend.bg.alpha = 0.9,
    legend.text.size = 0.6,
    legend.title.size = 0.6,
    inner.margins = c(0.1, 0.15, 0.1, 0.15)
  )

coefPCTVACANT

#tmap_save(coefPCTVACANT, filename = "HW2_Plots/coefPCTVACANT_Map.png", width = 8, height = 8, dpi = 300)
```

```{r}
arranged_coef <- tmap_arrange(coefLNNBELPOV, coefPCTBACHMOR, coefPCTSINGLES, coefPCTVACANT, ncol = 4)

arranged_coef

tmap_save(arranged_coef, filename = "HW2_Plots/arranged_coef_Map_columns.png", width = 8, height = 8, dpi = 300)
```

And we can also look at the spatial distribution of the local R-squares. We can see that the two predictors do a good job explaining the variance in our dependent variable in NW Philadelphia, but not in many other parts of the city.

```{r}
map <- tm_shape(regress_data)+
  tm_fill(col='localR2', 
          breaks=c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7), 
          n=5, 
          palette = 'Blues',
          title = expression(Local~R^2)) +  # Add custom legend title
  tm_borders(col = "white", lwd = 0.5) +
  tm_layout(frame=FALSE)

map
```

```{r}
# Save it
#tmap_save(map, filename = "HW2_Plots/LocalR2_Map.png", width = 8, height = 8, dpi = 300)
```

Finally, we can also look at the spatial autocorrelation in GWR residuals. This exercise is left for the student (spoiler alert: there’s much less autocorrelation in GWR residuals than in OLS residuals). Note that the residuals are extracted above using the command shp$gwrE<-gwrresults$gwr.e.

## 9. Model comparison

We can do the following to compare all 4 models: 1. Look at the Moran’s I of the residuals from each model and choose the model with the lowest (absolute) Moran’s I 2. Compare the AIC values from each model and choose the model with the lowest value

```{r}
# Compute Moran's I of residuals for each model to measures remaining spatial autocorrelation. 
# The closer the Moran's I statistic is to 0, the better.

moran_ols <- moran.mc(standardised_res, queenlist, 999, alternative = "two.sided")
moran_lag <- moran.mc(lag_reg$residuals, queenlist, 999, alternative = "two.sided")
moran_error <- moran.mc(error_reg$residuals, queenlist, 999, alternative = "two.sided")
moran_gwr <- moran.mc(gwr_model$SDF$gwr.e, queenlist, 999, alternative = "two.sided")
```

```{r}
moran_ols 
moran_lag 
moran_error
moran_gwr
```

In addition, we can do the following to compare the Spatial Lag Model to OLS: 1. Compare the log likelihoods; the model with the higher value is the better one. 2. Examine the results of the likelihood ratio test; if it’s significant, the Spatial Lag Model is better than OLS.

```{r}
# Compare AIC values. 
# The lower the AIC, the better the model fit (after penalization).

aic_ols <- AIC(ols_reg)
aic_lag <- AIC(lag_reg)
aic_error <- AIC(error_reg)
aic_gwr <- gwr_model$results$AICc   # GWR reports a corrected AIC (AICc)
```

```{r}
aic_ols
aic_lag
aic_error
aic_gwr
```

In addition, we can do the following to compare the Spatial Error Model to OLS:

1.  Compare the log likelihoods; the model with the higher value is the better one.
2.  Examine the results of the likelihood ratio test; if it’s significant, the Spatial Error Model is better than OLS.

```{r}
# Compare Log-Likelihoods and Likelihood Ratio Tests.
# Higher log-likelihood = better model fit.

loglik_ols   <- logLik(ols_reg)
loglik_lag   <- logLik(lag_reg)
loglik_error <- logLik(error_reg)

# Likelihood ratio tests
lr_lag_vs_ols   <- LR.Sarlm(lag_reg, ols_reg)
lr_error_vs_ols <- LR.Sarlm(error_reg, ols_reg)
```

```{r}
lr_lag_vs_ols
lr_error_vs_ols
# Significant LR test (p < 0.05) = spatial model significantly improves on OLS.
```

Lastly, we can also do the following to compare GWR to OLS:

1.Compare the R2 from OLS with the Quasi-global R2 from GWR; the model with the higher value is the better one. 2. Examine whether there’s spatial variability in standardized coefficients or local R2 values. If there is, it might mean that a single global regression might not capture the varying spatial relationships between the dependent variable and the predictors.

```{r}
# Compare R-squared values (OLS vs. GWR) GWR = quasi-global R-squared to compare with OLS.
r2_ols <- summary(ols_reg)$r.squared 

# OLS comes with the R-squared value, but gwr does not. We need to calculate the R-squared value
y <- regress_data$LNMEDHVAL # Dependent variable vector
r2_gwr <- 1 - (gwr_model$results$rss / sum((y - mean(y))^2))

```

```{r}
r2_ols
r2_gwr
# Higher R² = model explains more variance.
```

```{r}
# Comparison summary table

comparison <- data.frame(
  Model = c("OLS", "Spatial Lag", "Spatial Error", "GWR (Adaptive)"),
  # AIC: Lower AIC indicates better model fit after penalization
  AIC = c(
    AIC(ols_reg),
    AIC(lag_reg),
    AIC(error_reg),
    gwr_model$results$AICc   # Use AICc from GWR
  ),
  # Log-likelihood: Higher value indicates better fit
  LogLik = c(
    as.numeric(logLik(ols_reg)),
    as.numeric(logLik(lag_reg)),
    as.numeric(logLik(error_reg)),
    NA   # LogLik not reported for GWR
  ),
  # R-squared: Proportion of variance explained
  R2 = c(
    summary(ols_reg)$r.squared,
    NA,  # Not reported for SAR/SEM
    NA,  # Not reported for SEM
    r2_gwr  # Quasi-global R2
  )
)

comparison
```

```{r}
# The R-squared of 0.63 for GWR is noticeably higher than OLS’s 0.44, suggesting that the relationships between median income and predictors vary across space. Mapping the local R-squared and standardized coefficients will reveal these spatial variations, confirming that a single global model doesn’t fully capture local dynamics.

#The OLS model explains about 44% of the variance in median household income. However, residual diagnostics and Moran’s I indicate significant spatial autocorrelation, suggesting that OLS does not fully capture the spatial patterns in the data. 

#The Spatial Lag and Spatial Error models improve model fit over OLS, as indicated by lower AIC values (1207.83 and 1271.80, respectively) and higher log-likelihoods.

# GWR with an adaptive bandwidth provides the best overall fit, with the lowest AIC (1069.12) and a R² of 0.63. This confirms that a single global regression is insufficient to capture the nuanced spatial relationships, and GWR is better suited for modeling local variation.
```
