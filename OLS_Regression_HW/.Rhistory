plots_combined <- plot_grid(plot_LNPCTBACHMORE, plot_LNNBELPOV100, plot_LNPCTVACANT, plot_LNPCTSINGLES, ncol = 2, nrow = 2)
# Display plots with annotation.
plots_combined + plot_annotation(
title = "NATURAL LOG: Median House Value Predictors",
subtitle = "Using % Bachelor's Degrees, # Households Below Poverty,\n% Vacant Units, % Detached Single-Family Homes"
)
#| echo: false
#| message: false
#| warning: false
# Fitted.
regress_predicted <- fitted(regress_analysis)
# Residuals.
regress_residuals <- residuals(regress_analysis)
# Standardized residuals.
regress_standard_residuals <- rstandard(regress_analysis)
# Creating new empty dataframe with 3 columns and 1,720 rows for above calculations to store and plot.
predicted_residual_data <- data.frame(matrix(NA, nrow = 1720, ncol = 3))
colnames(predicted_residual_data) <- c("R_PREDICTED", "R_RESIDUALS", "R_STANDARD")
# Store calculated values into new dataframe.
predicted_residual_data <- predicted_residual_data %>%
mutate(
R_PREDICTED = regress_predicted,
R_RESIDUALS = regress_residuals,
R_STANDARD = regress_standard_residuals
)
# Histogram of standardized residuals.
hist_residuals <- ggplot(predicted_residual_data, aes(x = R_STANDARD)) +
geom_histogram(bins = 30, fill = "midnightblue", color = "white", alpha = 0.8) +
labs(title = "Histogram of Standardized Residuals",
x = "Standardized Residuals",
y = "Count") +
theme_minimal()
hist_residuals
#| echo: false
# Scatter plot.
predicted_residual_plot <- ggplot(predicted_residual_data, aes(x = R_PREDICTED, y = R_STANDARD)) +
geom_point(size = 2, alpha = 0.3, color = "midnightblue") +
labs(title = "Predicted Values vs. Standardized Residuals",
x = "Predicted Values",
y = "Standardized Residuals") +
theme_gray()
predicted_residual_plot
shapefile$residuals_standardized <- predicted_residual_data$R_STANDARD
#| echo: false
choropleth_residuals <- ggplot() +
geom_sf(data = shapefile, aes(fill = residuals_standardized), color = "cornsilk2", linewidth = 0.25) +
scale_fill_viridis_c(option = "viridis") +
labs(title = "Choropleth of Standardized Regression Residuals",
fill = "Standardized Residuals") +
theme_void()
choropleth_residuals
ggsave("choropleth_standardized_residuals.png", plot = choropleth_residuals, width = 8.5, height = 11, dpi = 600)
step_model <- step(regress_analysis, direction = "both")
step_model$anova
#| echo: false
# Model 1
fit1 <- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES, data = regress_data)
#summary(fit1)
#anova(fit1)
cv1 <- CVlm(data = regress_data, form.lm = fit1, m = 5, plotit = FALSE)
# Extract MSE and compute RMSE
mse1 <- attr(cv1, "ms")
rmse1 <- sqrt(mse1)
rmse1
#| echo: false
# Model 2: LNMEDHVAL ~ PCTVACANT + MEDHHINC
fit2 <- lm(LNMEDHVAL ~ PCTVACANT + MEDHHINC, data = regress_data)
#summary(fit2)
#anova(fit2)
cv2 <- CVlm(data = regress_data, form.lm = fit2, m = 5, plotit = FALSE)
mse2 <- attr(cv2, "ms")
rmse2 <- sqrt(mse2)
rmse2
#Read the Regression Data shp
regress_data <- st_read("data//RegressionData.shp")
#Read the Regression Data shp
regress_data <- st_read("data/RegressionData.shp")
#| eval: false
#| echo: true
library(sf)
library(dplyr)
library(tibble)
library(spdep)
library(spgwr)
library(tmap)
library(spatialreg)
library(whitestrap)
library(lmtest)
library(tseries)
options(scipen=999)
#Read the Regression Data shp
regress_data <- st_read("data/RegressionData.shp")
#Create variable PLUS1, defined as NBELPOV100+1
#We already have LNNBELPOV and LNMEDHVAL in the shapefile (1.a.iv, 1.a.v in hw instructions)
regress_data <- regress_data %>%
mutate(
PLUS1 = NBelPov100 + 1,
)
#Before proceeding, let’s examine the distributions of our variables and see whether we need to create log transformations of any of them
par(oma=c(0,0,2,0))
par(mfrow=c(1,3))
hist(regress_data$MEDHHINC, breaks = 50)
hist(regress_data$MEDHVAL, breaks = 50)
hist(regress_data$PCTVACANT, breaks = 50)
#PCTVANCANT has a spike at 0, so not transformation for them. MEDHHINC and MEDHVAL will be transformed/they already exist in the regress_data gdf.
#Graph of LNMED(HH)INC, LNMEDHVAL, and PCTVACANT
par(oma=c(0,0,2,0))
par(mfrow=c(1,3))
hist(regress_data$LNMEDINC, breaks = 50)
hist(regress_data$LNMEDHVAL, breaks = 50)
hist(regress_data$PCTVACANT, breaks = 50)
#I am making an assumption that LNMEDINC in the log of MEDHHINC. To reiterate, the LNMEDINC and LNMEDVHAL are present when you load the regress_data shp.
#defining neighbors for each of the block groups in philly using queen neighbors
queen_neighbors <- poly2nb(regress_data, row.names = regress_data$POLY_ID)
summary(queen_neighbors)
#It is useful to examine where block groups with non-average neighbor patterns are situated before we run spatial analyses. This is because these outlier block groups will affect our spatial analyses. We will look at the block groups where there is only 1 neighbor, and block groups where there are 27 neighbors. The block groups themselves will be colored in red, and their neighbors in green
#see which region has only one neighbor
smallestnbcard<-card(queen_neighbors) #extract neighbor matrix
smallestnb<-which(smallestnbcard == min(smallestnbcard)) #extract block groups with smallest number of neighbors
fg<-rep('grey90', length(smallestnbcard))
fg[smallestnb]<-'red' #color block groups red
fg[queen_neighbors[[smallestnb[1]]]]<-'green' #color neighboring blocks green
fg[queen_neighbors[[smallestnb[2]]]]<-'green'
fg[queen_neighbors[[smallestnb[3]]]]<-'green'
fg[queen_neighbors[[smallestnb[4]]]]<-'green'
plot(regress_data$geometry, col=fg)
title(main='Regions with only 1 neighbor')
#block group with most neighbors
largestnbcard<-card(queen_neighbors)
largestnb<-which(largestnbcard == max(largestnbcard))
fg1<-rep('grey90', length(largestnbcard))
fg1[largestnb]<-'red'
fg1[queen_neighbors[[largestnb]]]<-'green'
plot(regress_data$geometry, col=fg1)
title(main='Region with 27 neighbors')
#Choosing W as the style parameter, which will perform row standardization.
queenlist <- nb2listw(queen_neighbors, style = "W")
moran(regress_data$LNMEDINC, queenlist, n=length(queenlist$neighbours), S0 =Szero(queenlist))$"I"
# Check if this measure is statistically significant
moranI_mc <- moran.mc(regress_data$LNMEDINC, queenlist, nsim=999, alternative="two.sided")  #We use 999 permutations
moranI_mc
# Histogram of the Moran’s I values from random permutations
moranI_mc_res<-moranI_mc$res
hist(moranI_mc_res, freq=10000000, nclass=100)   #Draws distribution of Moran's I's calculated from randomly permuted values
# Here, we draw a red vertical line at the observed value of our Moran's I
abline(v=moran(regress_data$LNMEDINC, queenlist, n=length(queenlist$neighbours), S0=Szero(queenlist))$`I`, col='red')
#Create Moran plot (lagged value against observed value)
moran.plot(regress_data$LNMEDINC, queenlist)
#Run local moran's I (LISA)
lmoran<-localmoran(regress_data$LNMEDINC, queenlist)
head(lmoran)
# Combine the original regression dataset with the Local Moran's I results so that each observation includes its corresponding local spatial statistic
lmoran_df <-cbind(regress_data, as.data.frame(lmoran))
library(tmap)
tmap_mode("plot")
#Obtaining the Local Moran's P-Values (two-sided)
regress_data$local_I <- lmoran[, "Pr(z != E(Ii))"]
library(sf)
regress_data <- st_make_valid(regress_data) #Sometimes necessary if projection is off
#Creating the LISA Clusters
moran_plot <- moran.plot(as.vector(scale(regress_data$LNMEDINC)), queenlist)
regress_data$quadrant <- NA
# high-high
regress_data[(moran_plot$x >= 0 & moran_plot$wx >= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 1
# low-low
regress_data[(moran_plot$x <= 0 & moran_plot$wx <= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 2
# high-low
regress_data[(moran_plot$x >= 0 & moran_plot$wx <= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 3
# low-high
regress_data[(moran_plot$x <= 0 & moran_plot$wx >= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 4
# non-significant
regress_data[(regress_data$local_I > 0.05), "quadrant"] <- 5
# LISA P-Value Map
p_vals <- tm_shape(regress_data) +
tm_polygons(col = "local_I", title = "",
breaks = c(-Inf, 0.001, 0.01, 0.05, Inf),
palette = c("darkblue", "blue", "lightblue", "white")) +
tm_layout(
legend.outside = TRUE,
legend.text.size = 1,
legend.title.size = 1,
fontfamily = "Arial",
title = "LISA P-Value Map",
title.size = 1.2,
frame = FALSE
)
p_vals
# LISA Cluster Map
lisa_clusters <- tm_shape(regress_data) +
tm_fill(col = "quadrant", title = "",
breaks = c(1, 2, 3, 4, 5, 6),
palette = c("red", "blue", "lightpink", "skyblue2", "white"),
labels = c("High-High", "Low-Low", "High-Low", "Low-High", "Non-significant")) +
tm_borders(alpha = 0.5) +
tm_layout(
frame = FALSE,
legend.outside = TRUE,
legend.text.size = 1,
legend.title.size = 1,
fontfamily = "Arial",
title = "LISA Cluster Map",
title.size = 1.2
)
lisa_clusters
# Fit an Ordinary Least Squares (OLS) regression model
# Dependent variable: LNMEDINC (log of median income)
# Independent variables: LNMEDHVAL (log of median home value) and PCTVACANT (percent of vacant housing)
ols_reg<-lm(formula=LNMEDINC ~ LNMEDHVAL + PCTVACANT, data=regress_data)
summary(ols_reg)
#Prints the log likelihood
logLik(ols_reg)
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest(ols_reg, studentize=FALSE)
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest(ols_reg)
#Prints the results of the White Test to assess whether heteroscedasticity is present (package: whitestrap)
white_test(ols_reg)
# Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(ols_reg$residuals)
# Compute standardized residuals from the OLS regression which makes residuals comparable across observations
standardised_res<-rstandard(ols_reg)
# For each observation, calculate the mean standardized residual of its neighbors to identify whether high or low residuals cluster spatially
resnb<-sapply(queen_neighbors, function(x) mean(standardised_res[x]))
regress_data$standardised_res <- standardised_res    #creating a new variable in the shapefile shp.
OLS.Residuals.Map<-tm_shape(regress_data)+
tm_fill(col='standardised_res', style='quantile', title='Standardized OLS Residuals',
palette ='Blues')+
tm_layout(frame=FALSE, title = 'Standardised OLS Residuals')
OLS.Residuals.Map
#Regressing residuals on their nearest neighbors.
regress_res_lm <- lm(formula=standardised_res ~ resnb)
summary(regress_res_lm)
# Perform a Monte Carlo test for Moran’s I on the standardized OLS residuals
# Arguments:
#  - standardised_res: residuals from the OLS model, standardized for comparability
#  - queenlist: the spatial weights list (queen contiguity)
#  - 999: number of random permutations (higher = more precise p-value)
#  - alternative="two.sided": tests both positive and negative spatial autocorrelation
moran.mc(standardised_res, queenlist, 999, alternative="two.sided")
# The plot shows the relationship between each observation’s residual and the average residuals of its neighboring observations
# Arguments:
#  - standardised_res: residuals from the OLS model, standardized for comparability
#  - queenlist: the spatial weights list (queen contiguity)
moran.plot(standardised_res, queenlist)
# Dependent variable: LNMEDINC (log of median income)
# Independent variables: LNMEDHVAL (log of median home value), PCTVACANT (percent vacant housing)
# Spatial weights: queenlist (based on queen contiguity)
lag_reg<-lagsarlm(formula=LNMEDINC ~ LNMEDHVAL + PCTVACANT, data=regress_data, queenlist)
summary(lag_reg)
LR.Sarlm(lag_reg, ols_reg) #Here lag_reg is the SL output; ols_reg is the OLS output
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(lag_reg, studentize=FALSE)
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(lag_reg)
#Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(lag_reg$residuals)
lag_res <- lag_reg$residuals
# This checks whether any spatial autocorrelation remains after accounting for the spatial lag
# Arguments:
#  - res_lag: residuals from the spatial lag model
#  - queenlist: spatial weights list (queen contiguity)
#  - 999: number of random permutations for the Monte Carlo simulation
#  - alternative="two.sided": tests both positive and negative spatial autocorrelation
lag_moran_mc <- moran.mc(lag_res, queenlist,999, alternative="two.sided")
lag_moran_mc
error_reg <- errorsarlm(formula=LNMEDINC ~ LNMEDHVAL + PCTVACANT, data=regress_data, queenlist)
# Extract residuals from the fitted spatial error model
error_res <- residuals(error_reg)
# Compute the mean residual value for each observation's neighbors
error_res_mean <- sapply(queen_neighbors, function(x) mean(error_res[x]))
summary(error_reg)
# Perform a likelihood ratio (LR) test comparing the spatial error model to OLS
#  - error_reg: fitted spatial error model (errorsarlm)
#  - ols_reg: fitted OLS model
LR.Sarlm(error_reg, ols_reg)
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(error_reg, studentize=FALSE)
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(error_reg)
#Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(error_reg$residuals)
# Perform a Monte Carlo test for Moran's I on the residuals of the spatial error model
#  - error_res: residuals from the spatial error model (errorsarlm)
#  - queenlist: spatial weights list (queen contiguity)
#  - 999: number of random permutations for the Monte Carlo simulation
#  - alternative="two.sided": tests for both positive and negative spatial autocorrelation
error_moran_mc<-moran.mc(error_res, queenlist, 999, alternative="two.sided")
error_moran_mc
# convert regress_data from df to spatial data
regress_spatial <- as(regress_data, 'Spatial')  #These analyses are easier to do when the data are of the SpatialPolygonsDataFrame class
class (regress_spatial)
# Select an adaptive bandwidth for Geographically Weighted Regression (GWR)
#   - formula: dependent variable (LNMEDINC) and predictors (LNMEDHVAL, PCTVACANT)
#   - data: regress_spatial must be a SpatialPolygonsDataFrame (converted from sf)
#   - method = "aic": uses Akaike Information Criterion to select the optimal bandwidth
#   - adapt = TRUE: uses an adaptive bandwidth (proportion of nearest neighbors)
#     instead of a fixed distance bandwidth
bandwidth <- gwr.sel(formula = LNMEDINC ~ LNMEDHVAL + PCTVACANT,
data = regress_spatial,
method = "aic",
adapt = TRUE)
#Read the Regression Data shp
regress_data <- st_read("data/RegressionData.shp")
#Create variable PLUS1, defined as NBELPOV100+1
#We already have LNNBELPOV and LNMEDHVAL in the shapefile (1.a.iv, 1.a.v in hw instructions)
regress_data <- regress_data %>%
mutate(
PLUS1 = NBelPov100 + 1,
)
#Before proceeding, let’s examine the distributions of our variables and see whether we need to create log transformations of any of them
par(oma=c(0,0,2,0))
par(mfrow=c(1,3))
hist(regress_data$MEDHHINC, breaks = 50)
hist(regress_data$MEDHVAL, breaks = 50)
hist(regress_data$PCTVACANT, breaks = 50)
#PCTVANCANT has a spike at 0, so not transformation for them. MEDHHINC and MEDHVAL will be transformed/they already exist in the regress_data gdf.
#Graph of LNMED(HH)INC, LNMEDHVAL, and PCTVACANT
par(oma=c(0,0,2,0))
par(mfrow=c(1,3))
hist(regress_data$LNMEDINC, breaks = 50)
hist(regress_data$LNMEDHVAL, breaks = 50)
hist(regress_data$PCTVACANT, breaks = 50)
#I am making an assumption that LNMEDINC in the log of MEDHHINC. To reiterate, the LNMEDINC and LNMEDVHAL are present when you load the regress_data shp.
#defining neighbors for each of the block groups in philly using queen neighbors
queen_neighbors <- poly2nb(regress_data, row.names = regress_data$POLY_ID)
summary(queen_neighbors)
#It is useful to examine where block groups with non-average neighbor patterns are situated before we run spatial analyses. This is because these outlier block groups will affect our spatial analyses. We will look at the block groups where there is only 1 neighbor, and block groups where there are 27 neighbors. The block groups themselves will be colored in red, and their neighbors in green
#see which region has only one neighbor
smallestnbcard<-card(queen_neighbors) #extract neighbor matrix
smallestnb<-which(smallestnbcard == min(smallestnbcard)) #extract block groups with smallest number of neighbors
fg<-rep('grey90', length(smallestnbcard))
fg[smallestnb]<-'red' #color block groups red
fg[queen_neighbors[[smallestnb[1]]]]<-'green' #color neighboring blocks green
fg[queen_neighbors[[smallestnb[2]]]]<-'green'
fg[queen_neighbors[[smallestnb[3]]]]<-'green'
fg[queen_neighbors[[smallestnb[4]]]]<-'green'
plot(regress_data$geometry, col=fg)
title(main='Regions with only 1 neighbor')
#block group with most neighbors
largestnbcard<-card(queen_neighbors)
largestnb<-which(largestnbcard == max(largestnbcard))
fg1<-rep('grey90', length(largestnbcard))
fg1[largestnb]<-'red'
fg1[queen_neighbors[[largestnb]]]<-'green'
plot(regress_data$geometry, col=fg1)
title(main='Region with 27 neighbors')
#Choosing W as the style parameter, which will perform row standardization.
queenlist <- nb2listw(queen_neighbors, style = "W")
moran(regress_data$LNMEDINC, queenlist, n=length(queenlist$neighbours), S0 =Szero(queenlist))$"I"
# Check if this measure is statistically significant
moranI_mc <- moran.mc(regress_data$LNMEDINC, queenlist, nsim=999, alternative="two.sided")  #We use 999 permutations
moranI_mc
# Histogram of the Moran’s I values from random permutations
moranI_mc_res<-moranI_mc$res
hist(moranI_mc_res, freq=10000000, nclass=100)   #Draws distribution of Moran's I's calculated from randomly permuted values
# Here, we draw a red vertical line at the observed value of our Moran's I
abline(v=moran(regress_data$LNMEDINC, queenlist, n=length(queenlist$neighbours), S0=Szero(queenlist))$`I`, col='red')
#Create Moran plot (lagged value against observed value)
moran.plot(regress_data$LNMEDINC, queenlist)
#Run local moran's I (LISA)
lmoran<-localmoran(regress_data$LNMEDINC, queenlist)
head(lmoran)
# Combine the original regression dataset with the Local Moran's I results so that each observation includes its corresponding local spatial statistic
lmoran_df <-cbind(regress_data, as.data.frame(lmoran))
library(tmap)
tmap_mode("plot")
#Obtaining the Local Moran's P-Values (two-sided)
regress_data$local_I <- lmoran[, "Pr(z != E(Ii))"]
library(sf)
regress_data <- st_make_valid(regress_data) #Sometimes necessary if projection is off
#Creating the LISA Clusters
moran_plot <- moran.plot(as.vector(scale(regress_data$LNMEDINC)), queenlist)
regress_data$quadrant <- NA
# high-high
regress_data[(moran_plot$x >= 0 & moran_plot$wx >= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 1
# low-low
regress_data[(moran_plot$x <= 0 & moran_plot$wx <= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 2
# high-low
regress_data[(moran_plot$x >= 0 & moran_plot$wx <= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 3
# low-high
regress_data[(moran_plot$x <= 0 & moran_plot$wx >= 0) & (regress_data$local_I <= 0.05), "quadrant"]<- 4
# non-significant
regress_data[(regress_data$local_I > 0.05), "quadrant"] <- 5
# LISA P-Value Map
p_vals <- tm_shape(regress_data) +
tm_polygons(col = "local_I", title = "",
breaks = c(-Inf, 0.001, 0.01, 0.05, Inf),
palette = c("darkblue", "blue", "lightblue", "white")) +
tm_layout(
legend.outside = TRUE,
legend.text.size = 1,
legend.title.size = 1,
fontfamily = "Arial",
title = "LISA P-Value Map",
title.size = 1.2,
frame = FALSE
)
p_vals
# LISA Cluster Map
lisa_clusters <- tm_shape(regress_data) +
tm_fill(col = "quadrant", title = "",
breaks = c(1, 2, 3, 4, 5, 6),
palette = c("red", "blue", "lightpink", "skyblue2", "white"),
labels = c("High-High", "Low-Low", "High-Low", "Low-High", "Non-significant")) +
tm_borders(alpha = 0.5) +
tm_layout(
frame = FALSE,
legend.outside = TRUE,
legend.text.size = 1,
legend.title.size = 1,
fontfamily = "Arial",
title = "LISA Cluster Map",
title.size = 1.2
)
lisa_clusters
# Fit an Ordinary Least Squares (OLS) regression model
# Dependent variable: LNMEDINC (log of median income)
# Independent variables: LNMEDHVAL (log of median home value) and PCTVACANT (percent of vacant housing)
ols_reg<-lm(formula=LNMEDINC ~ LNMEDHVAL + PCTVACANT, data=regress_data)
summary(ols_reg)
#Prints the log likelihood
logLik(ols_reg)
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest(ols_reg, studentize=FALSE)
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest(ols_reg)
#Prints the results of the White Test to assess whether heteroscedasticity is present (package: whitestrap)
white_test(ols_reg)
# Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(ols_reg$residuals)
# Compute standardized residuals from the OLS regression which makes residuals comparable across observations
standardised_res<-rstandard(ols_reg)
# For each observation, calculate the mean standardized residual of its neighbors to identify whether high or low residuals cluster spatially
resnb<-sapply(queen_neighbors, function(x) mean(standardised_res[x]))
regress_data$standardised_res <- standardised_res    #creating a new variable in the shapefile shp.
OLS.Residuals.Map<-tm_shape(regress_data)+
tm_fill(col='standardised_res', style='quantile', title='Standardized OLS Residuals',
palette ='Blues')+
tm_layout(frame=FALSE, title = 'Standardised OLS Residuals')
OLS.Residuals.Map
#Regressing residuals on their nearest neighbors.
regress_res_lm <- lm(formula=standardised_res ~ resnb)
summary(regress_res_lm)
# Perform a Monte Carlo test for Moran’s I on the standardized OLS residuals
# Arguments:
#  - standardised_res: residuals from the OLS model, standardized for comparability
#  - queenlist: the spatial weights list (queen contiguity)
#  - 999: number of random permutations (higher = more precise p-value)
#  - alternative="two.sided": tests both positive and negative spatial autocorrelation
moran.mc(standardised_res, queenlist, 999, alternative="two.sided")
# The plot shows the relationship between each observation’s residual and the average residuals of its neighboring observations
# Arguments:
#  - standardised_res: residuals from the OLS model, standardized for comparability
#  - queenlist: the spatial weights list (queen contiguity)
moran.plot(standardised_res, queenlist)
# Dependent variable: LNMEDINC (log of median income)
# Independent variables: LNMEDHVAL (log of median home value), PCTVACANT (percent vacant housing)
# Spatial weights: queenlist (based on queen contiguity)
lag_reg<-lagsarlm(formula=LNMEDINC ~ LNMEDHVAL + PCTVACANT, data=regress_data, queenlist)
summary(lag_reg)
LR.Sarlm(lag_reg, ols_reg) #Here lag_reg is the SL output; ols_reg is the OLS output
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(lag_reg, studentize=FALSE)
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(lag_reg)
#Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(lag_reg$residuals)
lag_res <- lag_reg$residuals
# This checks whether any spatial autocorrelation remains after accounting for the spatial lag
# Arguments:
#  - res_lag: residuals from the spatial lag model
#  - queenlist: spatial weights list (queen contiguity)
#  - 999: number of random permutations for the Monte Carlo simulation
#  - alternative="two.sided": tests both positive and negative spatial autocorrelation
lag_moran_mc <- moran.mc(lag_res, queenlist,999, alternative="two.sided")
lag_moran_mc
error_reg <- errorsarlm(formula=LNMEDINC ~ LNMEDHVAL + PCTVACANT, data=regress_data, queenlist)
# Extract residuals from the fitted spatial error model
error_res <- residuals(error_reg)
# Compute the mean residual value for each observation's neighbors
error_res_mean <- sapply(queen_neighbors, function(x) mean(error_res[x]))
summary(error_reg)
# Perform a likelihood ratio (LR) test comparing the spatial error model to OLS
#  - error_reg: fitted spatial error model (errorsarlm)
#  - ols_reg: fitted OLS model
LR.Sarlm(error_reg, ols_reg)
#Prints the results of the Breusch-Pagan Test to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(error_reg, studentize=FALSE)
#Prints the results of the Koenker-Bassett Test (also known as the Studentized Breusch-Pagan Test) to assess whether heteroscedasticity is present (package: lmtest)
bptest.Sarlm(error_reg)
#Prints the results of the Jarque-Bera Test to assess whether residuals are normal (package: tseries)
jarque.bera.test(error_reg$residuals)
# Perform a Monte Carlo test for Moran's I on the residuals of the spatial error model
#  - error_res: residuals from the spatial error model (errorsarlm)
#  - queenlist: spatial weights list (queen contiguity)
#  - 999: number of random permutations for the Monte Carlo simulation
#  - alternative="two.sided": tests for both positive and negative spatial autocorrelation
error_moran_mc<-moran.mc(error_res, queenlist, 999, alternative="two.sided")
error_moran_mc
# convert regress_data from df to spatial data
regress_spatial <- as(regress_data, 'Spatial')  #These analyses are easier to do when the data are of the SpatialPolygonsDataFrame class
class (regress_spatial)
# Select an adaptive bandwidth for Geographically Weighted Regression (GWR)
#   - formula: dependent variable (LNMEDINC) and predictors (LNMEDHVAL, PCTVACANT)
#   - data: regress_spatial must be a SpatialPolygonsDataFrame (converted from sf)
#   - method = "aic": uses Akaike Information Criterion to select the optimal bandwidth
#   - adapt = TRUE: uses an adaptive bandwidth (proportion of nearest neighbors)
#     instead of a fixed distance bandwidth
bandwidth <- gwr.sel(formula = LNMEDINC ~ LNMEDHVAL + PCTVACANT,
data = regress_spatial,
method = "aic",
adapt = TRUE)
