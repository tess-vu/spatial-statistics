---
title: "Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia"
author: "Alex Stauffer, Jun Luu, Tess Vu"
date: today
output: html_document
---

# 1. INTRODUCTION

-   **INSTRUCTIONS:** State the problem and the setting of the analysis (i.e., Philadelphia).
-   **INSTRUCTIONS:** Present either a brief review of the literature (use Google Scholar) or simply speculate as to why the predictors we’re using might be related with the response variable.

The task is to explore the relationship between median house values and various neighborhood characteristics using Philadelphia data at the Census block group level. (Literature review on impact predictors have on response variable)

------------------------------------------------------------------------

# 2. METHODS

## a) Data Cleaning

The original Philadelphia block group data set had 1,816 observations. The data was cleaned by removing the following block groups:

```         
1) Block groups where population < 40
2) Block groups where there are no housing units
3) Block groups where the median house value is lower than $10,000
4) One North Philadelphia block group which had a very high median house value (over 800,000 USD) and a very low median household income (less than 8,000 USD)
```

The final data set that was tested consists of 1,720 block groups.

## b) Exploratory Data Analysis

Our purpose is to examine the summary statistics and distributions of the data. To achieve this, we start by creating histograms of each of the predictors to check for normal distribution and examining significant outliers.

As part of our exploratory data analysis, we also examine the correlations between the predictors to assess relationships among independent variables.

A correlation coefficient is a statistical measure that quantifies the strength and direction of the linear relationship between two variables. The sample correlation coefficient r is calculated as: 

$$
r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
$$ 
where x_i and y_i represent individual observations of the sample data and \bar{x} and \bar{y} are the sample means. 

The value of correlation coefficient r ranges from −1 to +1. A value of r = +1 indicates a perfect positive linear relationship, in that all (x_i, y_i) pairs lie on a line with a positive slope, and r = −1 indicates a perfect negative linear relationship, in that all (x_i, y_i) pairs lie on a line with a negative slope. A value of r = 0 indicates a lack of linear relationship between the variables.     


## c) Multiple Regression Analysis

-   **INSTRUCTIONS:** State the equation for y for this problem.
    -   In your report, instead of y and x1…xk, fill in the actual variable names (as in the regression example given above). Be sure to mention what βi’s and ε are as well. If the variables are log transformed, be sure to indicate that in the formulas.
-   **INSTRUCTIONS:** State and explain regression assumptions (e.g., linearity; independence of observations; normality of residuals; homoscedasticity; no multicollinearity).
-   **INSTRUCTIONS:** Mention the parameters that need to be estimated in multiple regression (σ2, β0 ,…, βk). State what σ2 is.
-   **INSTRUCTIONS:** Talk about the way of estimating the parameters. (Hint: present the equation on the slide ‘β Coefficient Estimation – Least Squares’ for multiple regression and briefly discuss what the equation does).
-   **INSTRUCTIONS:** Talk about the coefficient of multiple determination R2, and the adjusted R2. Present and explain the relevant formulas and all the terms that are used in the formulas.
-   **INSTRUCTIONS:** State the hypotheses you test. Specifically, talk about the F-ratio and the H0 and Ha associated with it, as well as the hypotheses you test about each of the individual βi’s (again, state H0 and Ha).

Multiple linear regression is a method used to examine the relationship between a dependent variable and one or more explanatory variables. In other words, this method visualizes the relationship between a dependent variable Y and a set of predictors X_1, X_2, ..., X_k. Regression is used to predict the dependent variable based on the predictors, to test hypotheses about relationships between variables, and to understand which predictors are important in explaining the dependent variable. Each independent variable has its own slope coefficient, which indicates the relationship of that particular predictor with the dependent variable while controlling for all other independent variables in the regression. However, it is important to note that if an explanatory variable is a significant predictor of the dependent variable this *does not* imply causation.

$$
\log(\text{Median Home Value}) = \beta_0 + \beta_1(\text{Poverty}) + \beta_2(\text{Education}) + \beta_3(\text{Vacancy}) + \beta_4(\text{Single Units}) + \varepsilon
$$
In this equation, β0 is the intercept, representing the expected value of median house value when all predictors equal zero. The coefficients βi (for i=1,2,…,k) are the partial regression coefficients, indicating the change in median house value associated with a one-unit increase in predictor i, holding all other predictors constant. The variable ε is the residual or random error term in the model. Without ε, any observed pair (x,y) would fall exactly on the line y=β0+β1x_1 called the true (or population) regression line. The inclusion of the random error term ε allows (x,y) observations to fall either above the true regression line (when ε>0) or below the line (when ε<0). The points (x1,y1),(x2,y2),…,(xn,yn) resulting from *n* independent observations will then be scattered about the true regression line.


## d) Additional Analyses

-   **INSTRUCTIONS:** Talk about stepwise regression – discuss what it does and its limitations.
-   **INSTRUCTIONS:** Talk about k-fold cross-validation (mentioning that k = 5) – discuss what it is used for, describe how it is operationalized and mention that the RMSE is used to compare models (explain what the RMSE is and how it is calculated, presenting and describing any relevant formulas).

## e) Software

-   **INSTRUCTIONS:** State that you’re using R for your data analysis.

All analyses were conducted in R Studio 2025.09.1+401 using R language version 4.5.1.

------------------------------------------------------------------------

# 3. RESULTS

## Exploratory Results

### Summary Statistics

-   **INSTRUCTIONS:** Present and briefly talk about the table with summary statistics which includes the dependent variable and the predictors (i.e., mean, standard deviation).

```{r}
#| message: false
#| warning: false
#| include: false
# Import relevant libraries.
library(MASS)
library(DAAG)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ggcorrplot)
library(patchwork)
library(cowplot)
library(sf)
library(knitr)
library(kableExtra)
library(gridExtra)

# No scientific notation.
options(scipen = 999)

# Read .csv file and store.
regress_data <- read.csv("RegressionData.csv")

# Turn to tibble.
regress_data <- as_tibble(regress_data)
```

```{r}
#| include: false
mean_MEDHVAL <- mean(regress_data[["MEDHVAL"]])
sd_MEDHVAL <- sd(regress_data[["MEDHVAL"]])

mean_PCTBACHMOR <- mean(regress_data[["PCTBACHMOR"]])
sd_PCTBACHMOR <- sd(regress_data[["PCTBACHMOR"]])

mean_NBELPOV100 <- mean(regress_data[["NBELPOV100"]])
sd_NBELPOV100 <- sd(regress_data[["NBELPOV100"]])

mean_PCTVACANT <- mean(regress_data[["PCTVACANT"]])
sd_PCTVACANT <- sd(regress_data[["PCTVACANT"]])

mean_PCTSINGLES <- mean(regress_data[["PCTSINGLES"]])
sd_PCTSINGLES <- sd(regress_data[["PCTSINGLES"]])
```

```{r}
#| echo: false
# Create data frame for summary statistics table.
sum_stats_table <- data.frame(VARIABLE = c("DEPENDENT VARIABLE", "Median House Value",
                                           "PREDICTORS", "# Households in Poverty",
                                           "% Individuals w/ Bachelor's Degrees or Higher",
                                           "% Vacant Houses", "% Single House Units"))

sum_stats_table <- data.frame(
  VARIABLE = c(
    "**DEPENDENT VARIABLE**", "Median House Value",
    "**PREDICTORS**", "# Households in Poverty",
    "% Individuals w/ Bachelor's Degrees or Higher",
    "% Vacant Houses", "% Single House Units"
  ),
  MEAN = c(
    "", sprintf("$%.2f", mean_MEDHVAL),
    "", sprintf("%.2f", mean_NBELPOV100),
    sprintf("%.2f%%", mean_PCTBACHMOR),
    sprintf("%.2f%%", mean_PCTVACANT),
    sprintf("%.2f%%", mean_PCTSINGLES)
  ),
  SD = c(
    "", sprintf("$%.2f", sd_MEDHVAL),
    "", sprintf("%.2f", sd_NBELPOV100),
    sprintf("%.2f%%", sd_PCTBACHMOR),
    sprintf("%.2f%%", sd_PCTVACANT),
    sprintf("%.2f%%", sd_PCTSINGLES)
  )
)

kable(sum_stats_table, digits = 2,
      caption = "<b>SUMMARY STATISTICS</b>",
      align = c("r", "c", "c"),
      format.args = list(big.mark = ",", nsmall = 2)
      ) %>%
  kable_styling(latex_options = "striped") %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, color = "white", background = "black")
```

All the variables' means and standard deviations are close together, with the bachelor's degree variable having a standard deviation higher than the mean. The data could be very inconsistent, because the standard deviations are as high or higher than the means this indicates the values fluctuate; this suggests there's a significant disparity in house values in the area. The average is influenced by outliers, so the data is dispersed far from the mean, it could be that the median would be best representative of the typical value in the area. It's possible the distribution *might not* be a normal, symmetrical bell curve.

### Histograms and Logarithmic Transformations

-   **INSTRUCTIONS:** State whether the variables are normal before and after the logarithmic transformation.
    -   Present the histograms of the original variables alongside the histograms of the log-transformed variables, and clearly state whether you’re using the log-transformed or original variable in your regression.
    -   State that the other regression assumptions will be examined in a separate section below (Regression Assumption Checks).

```{r}
#| include: false
# Add new columns at end of tibble dataframe that takes the natural logarithm of median house value (y, response variable) and its x, or predictor variables.
regress_data <- regress_data %>%
  mutate(
    "LNMEDHVAL" = log(MEDHVAL), # No zero values, can use normal log function.
    # Below have 0 values, use log function adding 1.
    "LNPCTBACHMORE" = log(1 + PCTBACHMOR),
    "LNNBELPOV100" = log(1 + NBELPOV100),
    "LNPCTVACANT" = log(1 + PCTVACANT),
    "LNPCTSINGLES" = log(1 + PCTSINGLES)
  )
```

```{r}
#| echo: false
#| message: false
#| warning: false
# Histogram of median household value
medhval_histogram <- ggplot(regress_data, aes(x = MEDHVAL)) +
  geom_histogram(fill = "blue2", color = "white") +
  scale_x_continuous(labels = scales::dollar_format(prefix = "$", big.mark = ",")) +
  labs(
    title = "Histogram of Median Household Value",
    x = "Median Household Value (Dollars, $)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Histogram of log-transformed values
lnmedhval_histogram <- ggplot(regress_data, aes(x = LNMEDHVAL)) +
  geom_histogram(fill = "orange2", color = "white") +
  labs(
    title = "Histogram of Log(Median Household Value)",
    x = "Log(Median Household Value)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Combine side by side
grid.arrange(medhval_histogram, lnmedhval_histogram, ncol = 2)
```

```{r}
#| echo: false
#| message: false
#| warning: false
# Histogram of percent bachelor's degrees or more.
pctbachmor_histogram <- ggplot(regress_data, aes(x = PCTBACHMOR)) +
  geom_histogram(fill = "blue2", color = "white") +
  scale_x_continuous(labels = scales::label_percent(scale = 1, big.mark = ",")) +
  labs(
    title = "Histogram of Percent Bachelor's+ Degrees",
    x = "Percent Bachelor's+ Degrees (Percent, %)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Histogram of log-transformed values.
lnpctbachmor_histogram <- ggplot(regress_data, aes(x = LNPCTBACHMORE)) +
  geom_histogram(fill = "orange2", color = "white") +
  labs(
    title = "Histogram of Log(Percent Bachelor's+ Degrees)",
    x = "Log(Percent Bachelor's+ Degrees)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Combine side by side
grid.arrange(pctbachmor_histogram, lnpctbachmor_histogram, ncol = 2)
```

```{r}
#| echo: false
#| message: false
#| warning: false
# Histogram of number of households in poverty.
nbelpov_histogram <- ggplot(regress_data, aes(x = NBELPOV100)) +
  geom_histogram(fill = "blue2", color = "white") +
  scale_x_continuous(labels = scales::comma) +
  labs(
    title = "Histogram of Number of Households in Poverty",
    x = "Number of Households in Poverty",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Histogram of log-transformed values.
lnbelpov_histogram <- ggplot(regress_data, aes(x = LNNBELPOV100)) +
  geom_histogram(fill = "orange2", color = "white") +
  labs(
    title = "Histogram of Log(Number of Households in Poverty)",
    x = "Log(Number of Households in Poverty)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Combine side by side
grid.arrange(nbelpov_histogram, lnbelpov_histogram, ncol = 2)
```

```{r}
#| echo: false
#| message: false
#| warning: false
# Histogram of percent vacancies.
pctbachmor_histogram <- ggplot(regress_data, aes(x = PCTVACANT)) +
  geom_histogram(fill = "blue2", color = "white") +
  scale_x_continuous(labels = scales::label_percent(scale = 1, big.mark = ",")) +
  labs(
    title = "Histogram of Percent Vacancies",
    x = "Percent Vacancies (Percent, %)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Histogram of log-transformed values.
lnpctbachmor_histogram <- ggplot(regress_data, aes(x = LNPCTVACANT)) +
  geom_histogram(fill = "orange2", color = "white") +
  labs(
    title = "Histogram of Log(Percent Vacancies)",
    x = "Log(Percent Vacancies)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Combine side by side
grid.arrange(pctbachmor_histogram, lnpctbachmor_histogram, ncol = 2)
```

```{r}
#| echo: false
#| message: false
#| warning: false
# Histogram of percent detached single house units.
pctbachmor_histogram <- ggplot(regress_data, aes(x = PCTSINGLES)) +
  geom_histogram(fill = "blue2", color = "white") +
  scale_x_continuous(labels = scales::label_percent(scale = 1, big.mark = ",")) +
  labs(
    title = "Histogram of Percent Detached Homes",
    x = "Percent Detached Homes (Percent, %)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Histogram of log-transformed values.
lnpctbachmor_histogram <- ggplot(regress_data, aes(x = LNPCTSINGLES)) +
  geom_histogram(fill = "orange2", color = "white") +
  labs(
    title = "Histogram of Log(Percent Detached Homes)",
    x = "Log(Percent Vacancies)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Combine side by side
grid.arrange(pctbachmor_histogram, lnpctbachmor_histogram, ncol = 2)
```

Other regression assumptions will be examined in a separate section below in "Regression Assumption Checks".

### Choropleth Maps

-   **INSTRUCTIONS:** Present the choropleth maps of the dependent variable and the predictors.
    -   Refer to the maps in the text, and talk about the following: Which maps look similar? Which maps look different? That is, which predictors do you expect to be strongly associated with the dependent variable based on the visualization? Also, given your examination of the maps, are there any predictors that you think will be strongly inter-correlated? That is, do you expect severe multicollinearity to be an issue here? Discuss this in a paragraph.

```{r}
#| echo: false
shapefile <- st_read("Lecture_1_RegressionData.shp/RegressionData.shp")

choropleth_LNMEDHVAL <- ggplot() +
  geom_sf(data = shapefile, aes(fill = LNMEDHVAL), color = "cornsilk2", linewidth = 0.25) +
  scale_fill_viridis_c() +
  labs(title = "Philadelphia Tracts: Log of Median House Value", fill = "Log of Median House Value") +
  theme_void()

choropleth_PCTVACANT <- ggplot() +
  geom_sf(data = shapefile, aes(fill = PCTVACANT), color = "cornsilk2", linewidth = 0.25) +
  scale_fill_viridis_c() +
  labs(fill = "% Vacant Units") +
  theme_void()

choropleth_PCTSINGLES <- ggplot() +
  geom_sf(data = shapefile, aes(fill = PCTSINGLES), color = "cornsilk2", linewidth = 0.25) +
  scale_fill_viridis_c() +
  labs(fill = "% Detached Single-Family Homes") +
  theme_void()

choropleth_PCTBACHMOR <- ggplot() +
  geom_sf(data = shapefile, aes(fill = PCTBACHMOR), color = "cornsilk2", linewidth = 0.25) +
  scale_fill_viridis_c() +
  labs(fill = "% Bachelor's Degrees") +
  theme_void()

choropleth_LNNBELPOV100 <- ggplot() +
  geom_sf(data = shapefile, aes(fill = LNNBELPOV), color = "black", linewidth = 0.25) +
  scale_fill_viridis_c() +
  labs(fill = "Households in Poverty") +
  theme_void()

plots_combined <- plot_grid(choropleth_PCTVACANT, choropleth_PCTSINGLES, choropleth_PCTBACHMOR, choropleth_LNNBELPOV100, ncol = 2, nrow = 2)

plots_combined + plot_annotation(
  title = "Philadelphia Tracts",
  subtitle = "Median House Value Predictors"
  )

#ggsave("choropleth_combined_map.png", plot = plots_combined, width = 14, height = 8.5, units = "in", dpi = 300)

plots_combined
```

-   House values are much higher in northwest Philadelphia, likely because it's a suburb and encompasses Wissahickon Valley Park's vast green space, and proximity to greenery tends to increase property values in neighborhoods.

-   Looks like there are more vacant units clustered in northern Philadelphia, which many local Philadelphians anecdotally state possesses high rates of low-income populations and communities in poverty. Can see a similar gradient, although less stark, in southwest Philadelphia and the Mantua neighborhood in west Philadelphia.

-   Less rowhouse architecture in the periphery of Philadelphia, which seems to spatially coincide with the visual that displays the natural log of median household values. The city is majorly connected rowhouses or apartment buildings.

-   High concentrations of those with a bachelor's degree or more residing in Center City, Wissahickon, and University City areas, as well as the northwestern part of the county on the west side of the Schuylkill River that leads toward Bryn Mawr.

-   Many households in poverty seem to encompass areas that don't have detached single-family homes. Lots of noticeable clusters in north, west, and south Philadelphia.

### Correlation Matrix

-   **INSTRUCTIONS:** Present the correlation matrix of the predictors which you obtained from R.
    -   Talk about whether the correlation matrix shows that there is severe multicollinearity.
    -   Does the correlation matrix support your conclusions based on your visual comparison of predictor maps?

```{r}
#| echo: false
correlation_matrix <- cor(regress_data[, c("LNPCTBACHMORE", "LNNBELPOV100", "LNPCTVACANT", "LNPCTSINGLES")],
                          use = "complete.obs",
                          method = "pearson")

print(correlation_matrix)

# Create a vector of new names in the desired order
#new_names <- c("ln(% Bachelor's +)", "ln(# Below Poverty)", "ln(% Vacancies)", "ln(% Detached)") # Adjust to your number of variables

# Assign the new names to the dimensions of the matrix
#colnames(correlation_matrix) <- new_names
#rownames(correlation_matrix) <- new_names

ggcorrplot(
  correlation_matrix,
  type = "full",
  lab = TRUE, # Add correlation coefficients
  lab_size = 3, # Label size
  digits = 2, # Decimal places
  hc.order = TRUE, # Hierarchical clustering for variable ordering
  outline.col = "white",
  ggtheme = ggplot2::theme_gray(),
  colors = c("#6D9EC1", "white", "#E46726")
)
```

**Observations:**

-   LNPCTBACHMORE = 0.4818251 (percent bachelor's degrees have a weak positive correlation with median household value)

-   LNNBELPOV100 = -0.3255461 (number households in poverty have a weak negative correlation with median household value)

-   LNPCTVACANT = -0.3205985 (percent vacant units have a weak negative correlation with median household value)

-   LNPCTSINGLES = 0.01660472 (percent detached single-family homes have a weak positive correlation with median household value).

0 ≤ \| r \| ≤ ±0.5: weak correlation

±0.5 \< \| r \| \< ±0.8: moderate correlation

±0.8 ≤ \| r \| \< ±1: strong correlation

## Regression Results

-   **INSTRUCTIONS:** Present the regression output from R. Be sure that your output presents the parameter estimates (and associated standard errors, t-statistics and p-values), as well as the R2, the adjusted R2, and the relevant F-ratio and associated p-value.
-   **INSTRUCTIONS:** Referencing the regression output, interpret the results as in the example included above this report outline.
-   **NOTE:** YOUR DEPENDENT VARIABLE (AND SOME PREDICTORS) WOULD BE LOG-TRANSFORMED. LOOK AT THE SLIDES FOR EXAMPLES OF INTERPRETING REGRESSION OUTPUT WITH LOG-TRANSFORMED VARIABLES.

```{r}
#| echo: false
# Regression analysis.
regress_analysis <- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100, regress_data)

summary(regress_analysis)
```

**Residuals Observations:** Residuals are differences between observed values and values predicted by regression model.

-   *Min:* Value of -2.25825 is smallest residual. Means largest underestimation made by model was by that many units.

-   *Q1 (First Quartile):* Value of -0.20391 means 25% of residuals are less than that value, and are points that are underestimations by the model.

-   *Median:* Value of 0.04735 means 50% of residuals are less than that value, and other 50% are higher. Median near zero suggests, on whole, model is *not* systematically overestimating or underestimating.

-   *Q3 (Third Quartile):* Value of 0.21744 means 75% of residuals are less than that value, and are points that are underestimations by the model.

-   *Max:* Value of 2.24347 is largest residual. Means largest overestimation made by model was by that many units.

**Coefficients Observations:**

-   *Estimate:* Indicates expected change in dependent variable LNMEDHVAL for each one-unit increase in respective independent variable.

-   *Intercept (Constant):* Y-intercept of regression line. Represents expected value of dependent variable when all independent variables are zero, so the dependent variable LNMEDHVAL is expected to be around 11.1137661.

    -   P-Value is less than 0.001, so the null hypothesis that the coefficient of Intercept is zero in the population is rejected.

-   *PCTVACANT:* If value of PCTVACANT changes by one unit, value of variable LNMEDHVAL changes by -0.0191569 units.

    -   P-Value is less than 0.001, so the null hypothesis that the coefficient of PCTVACANT is zero in the population is rejected.

-   *PCTSINGLES:* If value of PCTSINGLES changes by one unit, value of variable LNMEDHVAL changes by 0.0029769 units.

    -   P-Value is less than 0.001, so the null hypothesis that the coefficient of PCTSINGLES is zero in the population is rejected.

-   *PCTBACHMORE:* If value of PCTBACHMOR changes by one unit, value of variable LNMEDHVAL changes by 0.0209098 units.

    -   P-Value is less than 0.001, so the null hypothesis that the coefficient of PCTBACHMORE is zero in the population is rejected.

-   *LNNBELPOV100:* If value of PCTBACHMOR changes by one unit, value of variable LNMEDHVAL changes by -0.0789054 units.

    -   P-Value is less than 0.001, so the null hypothesis that the coefficient of LNNBELPOV100 is zero in the population is rejected.

**R Observations:**

-   *R (Correlation Coefficient):* Correlation coefficient between observed values of dependent variable ln(median house value) and predictions made by model using independent variables. R value of \~0.81 (derived from taking square root of "multiple R-squared" or "adjusted R-squared") indicates very high positive correlation between observed values and prediction made by model.

-   *R\^2 (R-Squared):* Proportion of variance in dependent variable that can be explained by independent variables in regression model. R\^2 value of 0.6623 means 66.23% of variance in dependent variable is explained by independent variables in model.

-   *Adjusted R\^2:* Adjusts R\^2 value based on number of variables in model and number of observations. More accurate measure when there are multiple independent variables, like in this case. Suggests that after adjusting for number of predictors, value of 0.6615 means about 66.15% of variance in dependent variable is accounted for.

-   *Standard Error of Estimate (Residual Standard Error):* Indicates average distance observed values fall from regression line (i.e. measure of accuracy of predictions made with regression model). Value of 0.3665 means that predicted values are, on average, 0.3665 units away from actual variables.

    -   NOTE: Whether this value is a small or large error depends on the context and scale of the dependent variable.

-   *Degrees of Freedom (df):* Indicates number of independent variables in model. There are 4 here, which are PCTVACANT (percent vacant units), PCTSINGLES (percent detached single-family homes), PCTBACHMOR (percent bachelor's degrees or more), and LNNBELPOV100 (log of number of households in poverty).

-   *F-Statistic (F):* Tests overall significance of model. Compares model with no predictors (only intercept, also referred to as constant) with model specified. Value of 819.09 used with degrees of freedom to calculate p-value.

-   *P-Value:* All values \<0.001, beyond the 0.001 threshold, so results are highly statistically significant. Can reject null hypothesis with high degree of confidence. Very unlikely observed results due to chance, indicating independent variables (predictors) in model have statistically significant effect on dependent variable LNMEDHVAL (log of median house values).

-   *Summary:* Model shows very high positive relationship between observed values and prediction, explains 66.15% of variance in dependent variable, but predictions are on average 0.3665 units away from actual values, which may or may not be significant depending on context of data. In this case, the log of median house value is used, and in statistics this means the natural log (i.e. Euler's constant *base e* or *ln(x)*) So that means increasing x (or predictor variable) by 1% is almost equivalent to adding 0.01 to ln(x) according to "Lecture 3-9" slide #144; *"For any variable x, small changes in ln(x) may be interpreted as % change in x"*.

## Regression Assumption Checks

### Scatter Plots of Dependent Variable and Predictors

-   **INSTRUCTIONS:** First state that in this section, you will be talking about testing model assumptions. State that you have already looked at the variable distributions earlier.
-   **INSTRUCTIONS:** Present scatter plots of the dependent variable and each of the predictors. State whether each of the relationships seems to be linear, as assumed by the regression model. \[Hint: they will not look linear.\]

```{r}
#| echo: false
# Scatter plot of ln(percent bachelor's degrees) vs. ln(median house value).
plot_LNPCTBACHMORE <- ggplot(regress_data, aes(x = LNPCTBACHMORE, y = LNMEDHVAL)) +
  geom_point(size = 2, alpha = 0.3, color = "midnightblue") +
  labs(title = "LOG: Percent Bachelor's Degree vs. Median House Value",
       x = "Percent Bachelor's Degree",
       y = "Median House Value") +
  theme_gray()

# Scatter plot of ln(number households in poverty) vs. ln(median house value).
plot_LNNBELPOV100 <- ggplot(regress_data, aes(x = LNNBELPOV100, y = LNMEDHVAL)) +
  geom_point(size = 2, alpha = 0.3, color = "midnightblue") +
  labs(title = "LOG: Number Households in Poverty vs. Median House Value",
       x = "Number Households in Poverty",
       y = "Median House Value") +
  theme_gray()

# Scatter plot of ln(percent vacant units) vs. ln(median house value).
plot_LNPCTVACANT <- ggplot(regress_data, aes(x = LNPCTVACANT, y = LNMEDHVAL)) +
  geom_point(size = 2, alpha = 0.3, color = "midnightblue") +
  labs(title = "LOG: Percent Vacant Units vs. Median House Value",
       x = "Percent Vacant Units",
       y = "Median House Value") +
  theme_gray()

# Scatter plot of ln(detached single-family homes) vs. ln(median house value).
plot_LNPCTSINGLES <- ggplot(regress_data, aes(x = LNPCTSINGLES, y = LNMEDHVAL)) +
  geom_point(size = 2, alpha = 0.3, color = "midnightblue") +
  labs(title = "LOG: Percent Detached Single-Family Homes vs. Median House Value",
       x = "Percent Detached Single-Family Homes",
       y = "Median House Value") +
  theme_gray()

# Create combined scatter plots with 2 rows and 2 columns.
plots_combined <- plot_grid(plot_LNPCTBACHMORE, plot_LNNBELPOV100, plot_LNPCTVACANT, plot_LNPCTSINGLES, ncol = 2, nrow = 2)

# Display plots with annotation.
plots_combined + plot_annotation(
  title = "NATURAL LOG: Median House Value Predictors",
  subtitle = "Using % Bachelor's Degrees, # Households Below Poverty,\n% Vacant Units, % Detached Single-Family Homes"
  )
```

### Histogram of Standardized Residuals

-   **INSTRUCTIONS:** Present the histogram of the standardized residuals. State whether the residuals look normal.

```{r}
#| echo: false
#| message: false
#| warning: false
# Fitted.
regress_predicted <- fitted(regress_analysis)

# Residuals.
regress_residuals <- residuals(regress_analysis)

# Standardized residuals.
regress_standard_residuals <- rstandard(regress_analysis)

# Creating new empty dataframe with 3 columns and 1,720 rows for above calculations to store and plot.
predicted_residual_data <- data.frame(matrix(NA, nrow = 1720, ncol = 3))
colnames(predicted_residual_data) <- c("R_PREDICTED", "R_RESIDUALS", "R_STANDARD")

# Store calculated values into new dataframe.
predicted_residual_data <- predicted_residual_data %>%
  mutate(
    R_PREDICTED = regress_predicted,
    R_RESIDUALS = regress_residuals,
    R_STANDARD = regress_standard_residuals
    )

# Histogram of standardized residuals.
hist_residuals <- ggplot(predicted_residual_data, aes(x = R_STANDARD)) +
  geom_histogram(bins = 30, fill = "midnightblue", color = "white", alpha = 0.8) +
  labs(title = "Histogram of Standardized Residuals",
       x = "Standardized Residuals",
       y = "Count") +
  theme_minimal()

hist_residuals
```

### Standardized Residual by Predicted Value Scatter Plot

-   **INSTRUCTIONS:** Present the ‘Standardized Residual by Predicted Value’ scatter plot. What conclusions can you draw from that? Does there seem to be heteroscedasticity? Do there seem to be outliers? Anything else? Discuss.
    -   Mention what standardized residuals are.

```{r}
#| echo: false
# Scatter plot.
predicted_residual_plot <- ggplot(predicted_residual_data, aes(x = R_PREDICTED, y = R_STANDARD)) + 
  geom_point(size = 2, alpha = 0.3, color = "midnightblue") +
  labs(title = "Predicted Values vs. Standardized Residuals", 
       x = "Predicted Values", 
       y = "Standardized Residuals") +
  theme_gray()

predicted_residual_plot
```

-   **INSTRUCTIONS:** Referencing the maps of the dependent variable and the predictors that you presented earlier, state whether there seems to be spatial autocorrelation in your variables. That is, does it seem that the observations (i.e., block groups) are independent of each other? Briefly discuss.

### Choropleth Map of Standardized Regression Residuals

-   **INSTRUCTIONS:** Now, present the choropleth map of the standardized regression residuals. Do there seem to be any noticeable spatial patterns in them? That is, do they seem to be spatially autocorrelated?
    -   You will examine the spatial autocorrelation of the variables and residuals and run spatial regressions in the next assignment.

```{r}
#| include: false
shapefile$residuals_standardized <- predicted_residual_data$R_STANDARD
```

```{r}
#| echo: false
choropleth_residuals <- ggplot() +
  geom_sf(data = shapefile, aes(fill = residuals_standardized), color = "cornsilk2", linewidth = 0.25) +
  scale_fill_viridis_c(option = "viridis") +
  labs(title = "Choropleth of Standardized Regression Residuals",
       fill = "Standardized Residuals") +
  theme_void()

choropleth_residuals
ggsave("choropleth_standardized_residuals.png", plot = choropleth_residuals, width = 8.5, height = 11, dpi = 600)
```

## Additional Models

### Stepwise Regression

-   **INSTRUCTIONS:** Present the results of the stepwise regression and state whether all 4 predictors in the original model are kept in the final model.

```{r}
#| echo: false
step_model <- step(regress_analysis, direction = "both")

step_model$anova
```

### Cross-Validation

-   **INSTRUCTIONS:** Present the cross-validation results – that is, compare the RMSE of the original model that includes all 4 predictors with the RMSE of the model that only includes PCTVACANT and MEDHHINC as predictors.

```{r}
#| echo: false
# Model 1
fit1 <- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES, data = regress_data)
#summary(fit1)
#anova(fit1)

cv1 <- CVlm(data = regress_data, form.lm = fit1, m = 5, plotit = FALSE)

# Extract MSE and compute RMSE
mse1 <- attr(cv1, "ms")
rmse1 <- sqrt(mse1)

rmse1
```

```{r}
#| echo: false
# Model 2: LNMEDHVAL ~ PCTVACANT + MEDHHINC
fit2 <- lm(LNMEDHVAL ~ PCTVACANT + MEDHHINC, data = regress_data)
#summary(fit2)
#anova(fit2)

cv2 <- CVlm(data = regress_data, form.lm = fit2, m = 5, plotit = FALSE)

mse2 <- attr(cv2, "ms")
rmse2 <- sqrt(mse2)

rmse2
```

------------------------------------------------------------------------

# 4. DISCUSSION AND LIMITATIONS

-   **INSTRUCTIONS:** Recap what you did in the paper and your findings. Discuss what conclusions you can draw, which variables were significant and whether that was surprising or not.
-   **INSTRUCTIONS:** Talk about the quality of the model – that is, state if this is a good model overall (e.g., R2, F-ratio test), and what other predictors that we didn’t include in our model might be associated with our dependent variable.
    -   Looking at the stepwise regression results, did the final model include all 4 predictors or were some dropped? What does that tell you about the quality of the model?
    -   Looking at the cross-validation results, was the RMSE better for the 4 predictor model or the 2 predictor model?
-   **INSTRUCTIONS:** If you haven’t done that in the Results section, talk explicitly about the limitations of the model – that is, mention which assumptions were violated, and if applicable, how that may affect the model/parameter estimation/estimated significance.
    -   In addition, talk about the limitations of using the NBELPOV100 variable as a predictor – that is, what are some limitations of using the raw number of households living in poverty rather than a percentage?
-   **INSTRUCTIONS:** Would it make sense to run Ridge or LASSO regression here? Explain briefly (\~4-5 sentences) what these methods are, when they’re used, and why they would or would not be appropriate here.

------------------------------------------------------------------------

# 5. WORKS CITED
