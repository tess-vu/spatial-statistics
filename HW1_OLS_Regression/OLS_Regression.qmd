---
title: "Using OLS Regression to Predict Median House Values in Philadelphia"
author:
  - Alex Stauffer
  - Jun Luu
  - Tess Vu
date: "October 19, 2025"
format: 
  html:
    toc: true
    theme: journal
  pdf:
    toc: true
knitr: 
  opts_chunk: 
    fig.path: "images/"
---

# 1. INTRODUCTION

The purpose of this analysis is to examine the relationship between median house values (MEDHVAL) and several neighborhood characteristics using Philadelphia data at the Census block group level. 

The dataset used for this study comes from the 2000 U.S. Census and includes 1,720 block groups after data cleaning to remove areas with little to no housing or extreme property values. Key variables in this analysis include the proportion of residents with at least a bachelorâ€™s degree (PCBACHMORE), the proportion of vacant housing units (PCTVACANT), the percentage of single-family detached homes (PCTSINGLES), the number of households living below the poverty line (NBELPOV100), and median household income (MEDHHINC). 

Higher education levels (PCBACHMORE) have been shown to correlate with higher income levels (MEDHHINC), which can lead to greater homeownership rates and increased housing prices in certain areas (Wang et al., 2022). Urban blight, defined as the presence of deteriorating, substandard, vacant, or abandoned properties, is shown through higher proportion of vacant units (PCTVACANT). This contributes to declining housing values, higher crime rates, and overall neighborhood disinvestment and distress (Bieretz & Schilling, 2019). 

By examining these predictor variables, this analysis seeks to better understand how neighborhood characteristics influence median house values across Philadelphia. 


------------------------------------------------------------------------

# 2. METHODS

## Data Cleaning 

The 2000 Philadelphia Census block group level dataset RegressionData.csv contains, among other variables, the variables below: 
1.	POLY_ID: Census Block Group ID. 
2.	MEDHVAL: Median value of all owner occupied housing units. 
3.	PCBACHMORE: Proportion of residents in Block Group with at least a bachelorâ€™s degree. 
4.	PCTVACANT: Proportion of housing units that are vacant. 
5.	PCTSINGLES: Percent of housing units that are detached single family houses. 
6.	NBELPOV100: Number of households with incomes below 100% poverty level (i.e., number of households living in poverty). 
7.	MEDHHINC: Median household income. 
  
The original Philadelphia block group dataset has 1,816 observations. The data was cleaned by removing the following block groups: 
1.	Block groups where population < 40. 
2.	Block groups where there are no housing units. 
3.	Block groups where the median house value is lower than $10,000. 
4.	One North Philadelphia block group which had a very high median house value (over $800,000) and a very low median household income (less than $8,000). 
  
Thereby achieving a data set with 1,720 observations. 


## Exploratory Data Analysis

Our purpose is to examine the summary statistics and distributions of the data. To achieve this, we start by creating histograms of each of the predictors to check for normal distribution and examining significant outliers.

As part of our exploratory data analysis, we also examine the correlations between the predictors to assess relationships among independent variables.

A correlation coefficient is a statistical measure that quantifies the strength and direction of the linear relationship between two variables. The sample correlation coefficient r is calculated as: 

$$
r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
$$ 
where x_i and y_i represent individual observations of the sample data and \bar{x} and \bar{y} are the sample means. 

The value of correlation coefficient r ranges from âˆ’1 to +1. A value of r = +1 indicates a perfect positive linear relationship, in that all (x_i, y_i) pairs lie on a line with a positive slope, and r = âˆ’1 indicates a perfect negative linear relationship, in that all (x_i, y_i) pairs lie on a line with a negative slope. A value of r = 0 indicates a lack of linear relationship between the variables.     


## Multiple Regression Analysis 

### Method of Regression 

Multiple linear regression is a method used to examine the relationship between a dependent, or response, variable and one or more independent, or predictor, variables. In other words, this method visualizes the relationship between dependent variable ğ‘¦ and a set of predictors ğ‘¥1,  ğ‘¥2, â€¦ , ğ‘¥ğ‘˜. Regression is used to predict the dependent variable based on the predictors, to test hypotheses about relationships between variables, and to understand which predictors are important in assessing the dependent variable. Each independent variable has its own slope coefficient, which indicates the relationship of that particular predictor with the dependent variable, while controlling for all other predictor variables in the regression. However, it is important to note that if an independent variable is a significant predictor of the dependent variable, this does not imply causation. 

### Equation 

The equation for this project is: 
 $$
\ln(\text{MEDHVAL}) = \beta_0 + \log\beta_1(\text{PCTVACANT}) + \beta_2(\text{PCTSINGLES}) + \beta_3(\text{PCTBACHMOR}) + \beta_4(\text{ln(NBELPOV100)}) + \varepsilon
 $$

Each independent variable in the model was assigned a slope coefficient ğ›½ğ‘–, which represents the relationship between that specific predictor and the dependent variable, controlling for all other predictors. In this context, each ğ›½ğ‘– quantifies the expected change in the dependent variable associated with a one-unit increase in the corresponding independent variable, holding the remaining variables constant. The intercept ğ›½0 represents the predicted value of the dependent variable when all predictors are equal to zero. The random error term or residual ğœ€ captures the unexplained variation in the dependent variableâ€”the difference between the observed and the predicted values based on the regression equation. 

### Regression Assumptions 

Multiple regression analysis relies on several key assumptions required for operation. The first assumption is that the relationship between the response variable and each of the predictor variables is linear. In other words, the change in a predictor variable maintains a constant and proportional relationship with the change in the dependent variable. By examining scatter plots of ğ‘¦ in relation to each predictor ğ‘¥ , this assumption can be verified. Should the relationship not be linear, variables can be logarithmically transformed, or a polynomial regression can be conducted.  

The second assumption maintains that all observations and residuals are independent, in other words, they should not be influenced or related to the other observationsâ€”residuals should not be correlated nor predict the next observation. Violations, such as temporal or spatial autocorrelation, can bias standard errors and lead to incorrect statistical inferences. 

The third assumption requires that the variance of the residuals is constant across all levels of the predictor variables, or homoscedasticity. This assumption is assessed visually using a scatter plot of standardized residuals by predicted values. 

The fourth assumption relies on the normality of residuals, in that the errors are normally distributed with a mean of zero. According to the Central Limit Theorem, the sampling distribution of the regression coefficients approaches normality as the sample size increases, which provides theoretical justification for this assumption even when individual observations are not normally distributed. 

The fifth assumption is that there is no multicollinearity among predictors, meaning the independent variables are not highly correlated with each other. To test multicollinearity, we can regress the predictor on all remaining predictors. If the correlation ğ‘…2 between predictors is ğ‘…2 > 0.8, this indicates the existence of multicollinearity. Furthermore, we can test the ğ‘…2 from above to calculate the Variance Inflation Factor (VIF). The VIF for each predictor k is defined as follows: 

$$
\text{VIF}_k = \frac{1}{1 - R_k^2}
$$
The general rule of thumb is that VIF values exceeding four warrant further investigation, while VIFs exceeding ten indicate serious multicollinearity, requiring correction.  

### Parameters to Estimate 

The multiple regression model requires estimation of ğ‘˜ + 2 parameters: the intercept ğ›½0, the 
ğ‘˜ regression coefficients ğ›½1, ğ›½2, â€¦ , ğ›½ğ‘˜, and the error variance ğœ2. In this case, they are ğ›½1, ğ›½2, ğ›½3, and ğ›½4, or PCTVACANT, PCTSINGLES, PCTBACHMOR, and LNNBELPOV100, 
respectively. The parameter ğœ2 determines the amount of variability within the regression model. If ğœ2is small, the observed pairs (ğ‘¥ğ‘–, ğ‘¦ğ‘–) will fall close to the true regression line. If ğœ2 is large, the observed pairs(ğ‘¥ğ‘–, ğ‘¦ğ‘–) will be spread out from the true regression line, reflecting greater unexplained variability. 

### Method of Estimating Parameters 

The method of estimating the parameters in multiple regression is the sum of squared errors. The least squares estimators for the regression coefficients are obtained with the below equation: 

$$

SEE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 

$$

Given ğ‘› observations on observed response value ğ‘¦ , and number of predictors ğ‘˜ and their observed values ğ‘¥1 â€¦ ğ‘¥ğ‘˜, the beta coefficient estimates ğ›½Ì‚0, ğ›½Ì‚1, ğ›½Ì‚2, â€¦ , ğ›½Ì‚ğ‘˜ are chosen simultaneously to minimize the expression for the Error Sum of Squares (SSE), given by the above equation. 
In this case, ğ›½Ì‚1, ğ›½Ì‚2, ğ›½Ì‚3, and ğ›½Ì‚4 all represent the estimated change in ğ‘¦ğ‘– when the given predictor increases by a single unit. 

### Coefficient of Determination (R^2) 

The coefficient of multiple determination ğ‘…2 is the proportion of variance explained by all ğ‘˜  predictors in the regression model, and generally, the more predictor variables there are in a model, the larger ğ‘…2 will be. The coefficient of multiple determination's equation is shown below: 
ğ‘…  
In the previous paragraph regarding an increase in the ğ‘…2 value being associated with an increase in the number of predictor variables, an adjustment to the ğ‘…2 value needs to be calculated, known as adjusted ğ‘…2, with the equation given below: 

$$
R^2=  1- \frac {SSE}{SST} =  1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$	
	
The resulting adjusted ğ‘…2 is the value reported; it is a more precise modification because it decreases when a predictor variable has a very marginal improvement to the model, meaning it penalizes any predictor variables deemed irrelevant. 

SSE is the Sum Squared Errors, which is the summation of the squared difference between Å· , which is the predicted value of ğ‘¦ , from each ğ‘¦ğ‘–, which is the observed value. This value is the amount of variance in ğ‘¦  that is unexplained by the regression model. SST is the Total Sum of Squares, which is the summation of the squared difference between ğ‘¦, the mean value of ğ‘¦ , from each ğ‘¦ğ‘–. This value is the total amount of variance in ğ‘¦ . So, the value when dividing SSE by SST is the proportion of total variance that is unexplained by the model, and when that value is subtracted from 1, the value is then the proportion of observed variation in the dependent variable ğ‘¦  explained by the model. The term ğ‘› is the number of observations in the sample, in this case it is 1,720. The term ğ‘˜ is the number of predictors in the model, in this case it is four. 

### Hypothesis Testing 

Two types of inferences test the hypotheses below: 

$$
ğ»_0 = ğ›½_1= ğ›½_2 = ğ›½_3 = ğ›½_4 = 0
$$
$$
ğ»_aâ€ˆ = ğ‘ğ‘¡ â€ˆğ‘™ğ‘’ğ‘ğ‘ ğ‘¡â€ˆ1â€ˆğ›½_iâ€ˆ â‰  0
$$

ğ»0 (null hypothesis) is where all coefficients are equal to zero, indicating no relationship. 
ğ»ğ‘ (alternative hypothesis) is where one of the predictor variables ğ›½ğ‘– does not equal zero, indicating at least one relationship between the response variable and a predictor variable. 

The F-Ratio, also known as the F-Test, is a preliminary step in hypothesis testing. The F-Ratio tests the ğ»0, where all coefficients in the model are all zero, versus the ğ»ğ‘, where at least one of the predictors is not zero. In other words, it compares the means among three or more groups, determining if at least one predictor is useful. A large F-Ratio is a strong case against the ğ»0, and determining if that value is statistically significant would come from the associated p-value being extremely small. 

The t-Test is also conducted for each predictor ğ‘– , and compares the means of the dependent and independent variables, that is, comparing their correlation and relationship with one another, and determining if they are statistically significant with the associated p-value. In other words, it compares the means between two groups, that of the predictor variable and the response variable. 

How these significance tests play with one another is as follows: the F-Test explains whether at least one predictor in the multiple regression model is useful. However, it does not explain what predictor that is, nor how many predictors are useful. So, the t-Test is conducted on each of the predictors ğ›½ğ‘– to determine their significance with the response variable ğ‘¦ . And again, an associated extremely small p-value would indicate a rejection of the ğ»0, where it is unlikely, but not impossible, that ğ»0 is true. 

## Additional Analyses 

### Stepwise Regression 

Stepwise regression is a model selection procedure that automatically selects a subset of predictor variables to include in the final model. The method iteratively adds or removes predictors based on statistical criteria, such as p-values or information criteria, to balance model complexity and fit. Stepwise regression can be useful for exploratory data analysis and for identifying the most important predictors. 

However, it has several limitations: it can be sensitive to multicollinearity, may select different models depending on the algorithm used (forward vs. backward selection), can result in biased parameter estimates and standard errors, and may capitalize on chance relationships in the data. Additionally, the statistical significance levels of individual coefficients in a stepwise model can be misleading. 

### k-Fold Cross-Validation 

K-fold cross-validation is used to evaluate a modelâ€™s quality. For this model, a five-fold crossvalidation (ğ‘˜ = 5 ) procedure was used. In this approach, the dataset was randomly divided into five approximately equal-sized folds. For each iteration, four folds were used to train the model, and the remaining fold served as the validation set. The Mean Squared Error (MSE) was calculated for the validation set, and this process was repeated five times so that each fold served as the validation set once. The overall MSE estimate is calculated by averaging the MSE values across the five folds. 
The Root Mean Squared Error (RMSE) was derived as the square root of this average MSE: 

$$
RMSE = \sqrt{\frac{\sum_{i=0}^{N - 1} (y_i - \hat{y}_i)^2}{N}}
$$

The RMSE measures a modelâ€™s prediction error in the same units as the dependent variable. Models are compared based on their RMSE values, with the model yielding the lowest considered the best performing. 

## Software 
All analyses were conducted in R Studio 2025.09.1+401 using R language version 4.5.1. 



------------------------------------------------------------------------

# 3. RESULTS

## Exploratory Results

### Summary Statistics


```{r}
#| message: false
#| warning: false
#| include: false
# Import relevant libraries.
library(MASS)
library(DAAG)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ggcorrplot)
library(patchwork)
library(cowplot)
library(sf)
library(knitr)
library(kableExtra)
library(gridExtra)

# No scientific notation.
options(scipen = 999)

# Read .csv file and store.
regress_data <- read.csv("RegressionData.csv")

# Turn to tibble.
regress_data <- as_tibble(regress_data)
```

```{r}
#| include: false
mean_MEDHVAL <- mean(regress_data[["MEDHVAL"]])
sd_MEDHVAL <- sd(regress_data[["MEDHVAL"]])

mean_PCTBACHMOR <- mean(regress_data[["PCTBACHMOR"]])
sd_PCTBACHMOR <- sd(regress_data[["PCTBACHMOR"]])

mean_NBELPOV100 <- mean(regress_data[["NBELPOV100"]])
sd_NBELPOV100 <- sd(regress_data[["NBELPOV100"]])

mean_PCTVACANT <- mean(regress_data[["PCTVACANT"]])
sd_PCTVACANT <- sd(regress_data[["PCTVACANT"]])

mean_PCTSINGLES <- mean(regress_data[["PCTSINGLES"]])
sd_PCTSINGLES <- sd(regress_data[["PCTSINGLES"]])
```

```{r}
#| echo: false
# Create data frame for summary statistics table.
sum_stats_table <- data.frame(VARIABLE = c("DEPENDENT VARIABLE", "Median House Value",
                                           "PREDICTORS", "# Households in Poverty",
                                           "% Individuals w/ Bachelor's Degrees or Higher",
                                           "% Vacant Houses", "% Single House Units"))

sum_stats_table <- data.frame(
  VARIABLE = c(
    "**DEPENDENT VARIABLE**", "Median House Value",
    "**PREDICTORS**", "# Households in Poverty",
    "% Individuals w/ Bachelor's Degrees or Higher",
    "% Vacant Houses", "% Single House Units"
  ),
  MEAN = c(
    "", sprintf("$%.2f", mean_MEDHVAL),
    "", sprintf("%.2f", mean_NBELPOV100),
    sprintf("%.2f%%", mean_PCTBACHMOR),
    sprintf("%.2f%%", mean_PCTVACANT),
    sprintf("%.2f%%", mean_PCTSINGLES)
  ),
  SD = c(
    "", sprintf("$%.2f", sd_MEDHVAL),
    "", sprintf("%.2f", sd_NBELPOV100),
    sprintf("%.2f%%", sd_PCTBACHMOR),
    sprintf("%.2f%%", sd_PCTVACANT),
    sprintf("%.2f%%", sd_PCTSINGLES)
  )
)

kable(sum_stats_table, digits = 2,
      caption = "<b>SUMMARY STATISTICS</b>",
      align = c("r", "c", "c"),
      format.args = list(big.mark = ",", nsmall = 2)
      ) %>%
  kable_styling(latex_options = "striped") %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, color = "white", background = "black")
```

The Summary Statistics Table 1 shows the dependent variable, median house value (MEDHVAL), and the predictor variables. Overall, the means and standard deviations are relatively close across variables, though the bachelorâ€™s degree variable (PCTBACHMOR) shows a standard deviation higher than its mean. This suggests considerable variability within the data. Since several variables have standard deviations as high as or higher than their means, the data may be quite inconsistent, indicating fluctuations and potential disparities in housing values across neighborhoods. The presence of large deviations also suggests that outliers may be influencing the averages, meaning the mean may not accurately represent the typical value. In this case, the median might better capture central tendencies, and the distribution may be skewed rather than normally distributed. 

### Histograms and Logarithmic Transformations


```{r}
#| include: false
# Add new columns at end of tibble dataframe that takes the natural logarithm of median house value (y, response variable) and its x, or predictor variables.
regress_data <- regress_data %>%
  mutate(
    "LNMEDHVAL" = log(MEDHVAL), # No zero values, can use normal log function.
    # Below have 0 values, use log function adding 1.
    "LNPCTBACHMORE" = log(1 + PCTBACHMOR),
    "LNNBELPOV100" = log(1 + NBELPOV100),
    "LNPCTVACANT" = log(1 + PCTVACANT),
    "LNPCTSINGLES" = log(1 + PCTSINGLES)
  )
```

```{r}
#| echo: false
#| message: false
#| warning: false
# Histogram of median household value
medhval_histogram <- ggplot(regress_data, aes(x = MEDHVAL)) +
  geom_histogram(fill = "blue2", color = "white") +
  scale_x_continuous(labels = scales::dollar_format(prefix = "$", big.mark = ",")) +
  labs(
    title = "Histogram of Median Household Value",
    x = "Median Household Value (Dollars, $)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Histogram of log-transformed values
lnmedhval_histogram <- ggplot(regress_data, aes(x = LNMEDHVAL)) +
  geom_histogram(fill = "orange2", color = "white") +
  labs(
    title = "Histogram of Log(Median Household Value)",
    x = "Log(Median Household Value)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Combine side by side
grid.arrange(medhval_histogram, lnmedhval_histogram, ncol = 2)
```

```{r}
#| echo: false
#| message: false
#| warning: false
# Histogram of percent bachelor's degrees or more.
pctbachmor_histogram <- ggplot(regress_data, aes(x = PCTBACHMOR)) +
  geom_histogram(fill = "blue2", color = "white") +
  scale_x_continuous(labels = scales::label_percent(scale = 1, big.mark = ",")) +
  labs(
    title = "Histogram of Percent Bachelor's+ Degrees",
    x = "Percent Bachelor's+ Degrees (Percent, %)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Histogram of log-transformed values.
lnpctbachmor_histogram <- ggplot(regress_data, aes(x = LNPCTBACHMORE)) +
  geom_histogram(fill = "orange2", color = "white") +
  labs(
    title = "Histogram of Log(Percent Bachelor's+ Degrees)",
    x = "Log(Percent Bachelor's+ Degrees)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Combine side by side
grid.arrange(pctbachmor_histogram, lnpctbachmor_histogram, ncol = 2)
```

```{r}
#| echo: false
#| message: false
#| warning: false
# Histogram of number of households in poverty.
nbelpov_histogram <- ggplot(regress_data, aes(x = NBELPOV100)) +
  geom_histogram(fill = "blue2", color = "white") +
  scale_x_continuous(labels = scales::comma) +
  labs(
    title = "Histogram of Number of Households in Poverty",
    x = "Number of Households in Poverty",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Histogram of log-transformed values.
lnbelpov_histogram <- ggplot(regress_data, aes(x = LNNBELPOV100)) +
  geom_histogram(fill = "orange2", color = "white") +
  labs(
    title = "Histogram of Log(Number of Households in Poverty)",
    x = "Log(Number of Households in Poverty)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Combine side by side
grid.arrange(nbelpov_histogram, lnbelpov_histogram, ncol = 2)
```

```{r}
#| echo: false
#| message: false
#| warning: false
# Histogram of percent vacancies.
pctbachmor_histogram <- ggplot(regress_data, aes(x = PCTVACANT)) +
  geom_histogram(fill = "blue2", color = "white") +
  scale_x_continuous(labels = scales::label_percent(scale = 1, big.mark = ",")) +
  labs(
    title = "Histogram of Percent Vacancies",
    x = "Percent Vacancies (Percent, %)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Histogram of log-transformed values.
lnpctbachmor_histogram <- ggplot(regress_data, aes(x = LNPCTVACANT)) +
  geom_histogram(fill = "orange2", color = "white") +
  labs(
    title = "Histogram of Log(Percent Vacancies)",
    x = "Log(Percent Vacancies)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Combine side by side
grid.arrange(pctbachmor_histogram, lnpctbachmor_histogram, ncol = 2)
```

```{r}
#| echo: false
#| message: false
#| warning: false
# Histogram of percent detached single house units.
pctbachmor_histogram <- ggplot(regress_data, aes(x = PCTSINGLES)) +
  geom_histogram(fill = "blue2", color = "white") +
  scale_x_continuous(labels = scales::label_percent(scale = 1, big.mark = ",")) +
  labs(
    title = "Histogram of Percent Detached Homes",
    x = "Percent Detached Homes (Percent, %)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Histogram of log-transformed values.
lnpctbachmor_histogram <- ggplot(regress_data, aes(x = LNPCTSINGLES)) +
  geom_histogram(fill = "orange2", color = "white") +
  labs(
    title = "Histogram of Log(Percent Detached Homes)",
    x = "Log(Percent Vacancies)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    axis.title.x = element_text(face = "italic", size = 10),
    axis.title.y = element_text(face = "italic", size = 10)
  )

# Combine side by side
grid.arrange(pctbachmor_histogram, lnpctbachmor_histogram, ncol = 2)
```

All variables were first graphed to review normality using histograms. None of the original variables appeared to follow a normal distribution, so all logarithmic transformations were applied to assess whether the distributions could be normalized.  

Overall, the logarithmic transformation substantially improved normality for the dependent variable (MEDHVAL), resulting in a roughly symmetric distribution for LNMEDHVAL. The only predictor that appeared more normally distributed after transformation was NBELPOV100. The other predictors, PCTBACHMOR, PCTVACANT, and PCTSINGLES, were still skewed and showed zero-inflated distributions even after transformation. 

An examination of the other regression assumptions will be discussed in the Regression Assumption Checks section below. 


### Choropleth Maps


```{r}
#| echo: false
shapefile <- st_read("data/RegressionData.shp")

choropleth_LNMEDHVAL <- ggplot() +
  geom_sf(data = shapefile, aes(fill = LNMEDHVAL), color = "cornsilk2", linewidth = 0.25) +
  scale_fill_viridis_c() +
  labs(title = "Philadelphia Tracts: Log of Median House Value", fill = "Log of Median House Value") +
  theme_void()

choropleth_PCTVACANT <- ggplot() +
  geom_sf(data = shapefile, aes(fill = PCTVACANT), color = "cornsilk2", linewidth = 0.25) +
  scale_fill_viridis_c() +
  labs(fill = "% Vacant Units") +
  theme_void()

choropleth_PCTSINGLES <- ggplot() +
  geom_sf(data = shapefile, aes(fill = PCTSINGLES), color = "cornsilk2", linewidth = 0.25) +
  scale_fill_viridis_c() +
  labs(fill = "% Detached Single-Family Homes") +
  theme_void()

choropleth_PCTBACHMOR <- ggplot() +
  geom_sf(data = shapefile, aes(fill = PCTBACHMOR), color = "cornsilk2", linewidth = 0.25) +
  scale_fill_viridis_c() +
  labs(fill = "% Bachelor's Degrees") +
  theme_void()

choropleth_LNNBELPOV100 <- ggplot() +
  geom_sf(data = shapefile, aes(fill = LNNBELPOV), color = "black", linewidth = 0.25) +
  scale_fill_viridis_c() +
  labs(fill = "Households in Poverty") +
  theme_void()

plots_combined <- plot_grid(choropleth_PCTVACANT, choropleth_PCTSINGLES, choropleth_PCTBACHMOR, choropleth_LNNBELPOV100, ncol = 2, nrow = 2)

plots_combined + plot_annotation(
  title = "Philadelphia Tracts",
  subtitle = "Median House Value Predictors"
  )

#ggsave("choropleth_combined_map.png", plot = plots_combined, width = 14, height = 8.5, units = "in", dpi = 300)

plots_combined
```


Visually, the maps for median house value, percentage of population with a bachelor's degree, and percentage of detached homes appear quite similar. There are higher values clustered towards the downtown area and decreasing as you move away from the city center, suggesting a strong positive relationship between income and housing value. Adversely, the percentage of vacant units and households below the poverty line have an inverse relationship. Based on these visual trends, income and education may be correlated with each other as well as with housing value, which raises the possibility of multicollinearity among these predictors. 


### Correlation Matrix


```{r}
#| echo: false
correlation_matrix <- cor(regress_data[, c("LNPCTBACHMORE", "LNNBELPOV100", "LNPCTVACANT", "LNPCTSINGLES")],
                          use = "complete.obs",
                          method = "pearson")

print(correlation_matrix)

# Create a vector of new names in the desired order
#new_names <- c("ln(% Bachelor's +)", "ln(# Below Poverty)", "ln(% Vacancies)", "ln(% Detached)") # Adjust to your number of variables

# Assign the new names to the dimensions of the matrix
#colnames(correlation_matrix) <- new_names
#rownames(correlation_matrix) <- new_names

ggcorrplot(
  correlation_matrix,
  type = "full",
  lab = TRUE, # Add correlation coefficients
  lab_size = 3, # Label size
  digits = 2, # Decimal places
  hc.order = TRUE, # Hierarchical clustering for variable ordering
  outline.col = "white",
  ggtheme = ggplot2::theme_gray(),
  colors = c("#6D9EC1", "white", "#E46726")
)
```

**Observations:**

The correlation matrix does not show severe multicollinearity, all coefficients within the figure have absolute values of 0.32 or less, indicating that the predictor variables have very minimal correlation with each other. Multicollinearity was considered a possibility when observing the choropleth mapsâ€™ spatial patterns and the pockets of inverted blocks between LNNBELPOV100 and predictor variables PCTBACHMOR and PCTSINGLES, but the 
correlation matrix indicates there is very little correlation among the predictor variables, and a VIF analysis could be conducted as another multicollinearity check. 


## Regression Results


```{r}
#| echo: false
# Regression analysis.
regress_analysis <- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV100, regress_data)

summary(regress_analysis)
```

The log of median household value (LNMEDHVAL) was regressed on the percent of vacant housing units (PCTVACANT), percent of detached single-family houses (PCTSINGLES), percent of residents with at least a bachelorâ€™s degree (PCTBACHMOR), and the log of number of households in poverty (LNNBELPOV100). Considering the log-transformed median household value response variable (LNMEDHVAL) and poverty predictor variable 
(LNNBELPOV100), a 0.01 change in their values may be interpreted as a percent change in the original, pre-transformed value. 

The regression output indicates that the percent of vacant housing units, percent of detached single-family homes, percent of residents with at least a bachelorâ€™s degree, and number of households in poverty are highly significant and are positively associated with median household value (p < 0.0001 for all variables). 

A one percent increase in the percentage of vacancies within the block group is associated with a rounded ğ›½1 = âˆ’0.0192 change, an approximately 1.92% decrease in median household value. A one percent increase in the percentage of detached single-family homes within the block group is associated with a rounded ğ›½2 = 0.0030 change, an approximately 0.03% marginal increase in median household value. A one percent increase in the percentage of residents with at least a bachelorâ€™s degree is associated with a ğ›½3 = 0.0209 change, an approximately 2.09% increase in median household value. A one percent increase in the number of those in poverty is associated with a rounded ğ›½4 = âˆ’0.0800 change, an approximately 0.08% decrease in median household value. And all given predictor beta coefficient changes are holding other predictors constant. 

The p-value of less than 0.0001 for all predictor variables indicates that if there is actually no relationship between the predictor variables and the dependent variable LNMEDHVAL (i.e. if the null hypotheses that ğ›½1 = ğ›½2 = ğ›½3 = ğ›½4 = 0 are actually true), then the probability of getting a ğ›½1 coefficient estimate of â€“0.0192, a ğ›½2 coefficient estimate of 0.0030, a ğ›½3 coefficient estimate of 0.0209, and a ğ›½4 coefficient estimate of â€“0.08 are all less than 0.0001. These low probabilities indicate that we can safely reject ğ»0: ğ›½1 = ğ›½2 = ğ›½3 = ğ›½4 = 0 for ğ»ğ‘: ğ»ğ‘ = ğ‘ğ‘¡â€ˆğ‘™ğ‘’ğ‘ğ‘ ğ‘¡â€ˆ1â€ˆğ›½ğ‘– â‰  0 (at most reasonable levels of ğ›¼ = ğ‘ƒ(ğ‘‡ğ‘¦ğ‘ğ‘’â€ˆğ¼â€ˆğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ)). 

66.15% of the variance in the dependent variable is explained by the model (ğ‘…2 and adjusted ğ‘…2 are 0.6623 and 0.6615, respectively). The low p-value less than 0.0001 associated with a large FRatio 840.9 shows that it is safe to reject the null hypothesis that all coefficients in the model are 0. 


## Regression Assumption Checks

### Scatter Plots of Dependent Variable and Predictors

This section discusses testing model assumptions, and variable distributions were analyzed earlier. 

```{r}
#| echo: false
# Scatter plot of ln(percent bachelor's degrees) vs. ln(median house value).
plot_LNPCTBACHMORE <- ggplot(regress_data, aes(x = LNPCTBACHMORE, y = LNMEDHVAL)) +
  geom_point(size = 2, alpha = 0.3, color = "midnightblue") +
  labs(title = "LOG: Percent Bachelor's Degree vs. Median House Value",
       x = "Percent Bachelor's Degree",
       y = "Median House Value") +
  theme_gray()

# Scatter plot of ln(number households in poverty) vs. ln(median house value).
plot_LNNBELPOV100 <- ggplot(regress_data, aes(x = LNNBELPOV100, y = LNMEDHVAL)) +
  geom_point(size = 2, alpha = 0.3, color = "midnightblue") +
  labs(title = "LOG: Number Households in Poverty vs. Median House Value",
       x = "Number Households in Poverty",
       y = "Median House Value") +
  theme_gray()

# Scatter plot of ln(percent vacant units) vs. ln(median house value).
plot_LNPCTVACANT <- ggplot(regress_data, aes(x = LNPCTVACANT, y = LNMEDHVAL)) +
  geom_point(size = 2, alpha = 0.3, color = "midnightblue") +
  labs(title = "LOG: Percent Vacant Units vs. Median House Value",
       x = "Percent Vacant Units",
       y = "Median House Value") +
  theme_gray()

# Scatter plot of ln(detached single-family homes) vs. ln(median house value).
plot_LNPCTSINGLES <- ggplot(regress_data, aes(x = LNPCTSINGLES, y = LNMEDHVAL)) +
  geom_point(size = 2, alpha = 0.3, color = "midnightblue") +
  labs(title = "LOG: Percent Detached Single-Family Homes vs. Median House Value",
       x = "Percent Detached Single-Family Homes",
       y = "Median House Value") +
  theme_gray()

# Create combined scatter plots with 2 rows and 2 columns.
plots_combined <- plot_grid(plot_LNPCTBACHMORE, plot_LNNBELPOV100, plot_LNPCTVACANT, plot_LNPCTSINGLES, ncol = 2, nrow = 2)

# Display plots with annotation.
plots_combined + plot_annotation(
  title = "NATURAL LOG: Median House Value Predictors",
  subtitle = "Using % Bachelor's Degrees, # Households Below Poverty,\n% Vacant Units, % Detached Single-Family Homes"
  )
```

This presents the scatter plots of each of the predictor variablesâ€™ relationships with the 
dependent variable LNMEDHVAL. PCTBACHMOR has an almost concave downward curve 
that rises again as the percentage of those who hold at least a bachelorâ€™s increases, 
LNNBELPOV100 is very clustered to one area of the graph between values four to six on the xaxis without any clear linearity, PCTVACANT has a downward sloping curve, and 
PCTSINGLES has a concentrated form from zero to 25 on the x-axis with several observations aligned horizontally around when y is 11.5 to 12.5. The multivariate regression model assumes relationship linearity; however, itâ€™s clear that none of these predictors are linear. 


### Histogram of Standardized Residuals


```{r}
#| echo: false
#| message: false
#| warning: false
# Fitted.
regress_predicted <- fitted(regress_analysis)

# Residuals.
regress_residuals <- residuals(regress_analysis)

# Standardized residuals.
regress_standard_residuals <- rstandard(regress_analysis)

# Creating new empty dataframe with 3 columns and 1,720 rows for above calculations to store and plot.
predicted_residual_data <- data.frame(matrix(NA, nrow = 1720, ncol = 3))
colnames(predicted_residual_data) <- c("R_PREDICTED", "R_RESIDUALS", "R_STANDARD")

# Store calculated values into new dataframe.
predicted_residual_data <- predicted_residual_data %>%
  mutate(
    R_PREDICTED = regress_predicted,
    R_RESIDUALS = regress_residuals,
    R_STANDARD = regress_standard_residuals
    )

# Histogram of standardized residuals.
hist_residuals <- ggplot(predicted_residual_data, aes(x = R_STANDARD)) +
  geom_histogram(bins = 30, fill = "midnightblue", color = "white", alpha = 0.8) +
  labs(title = "Histogram of Standardized Residuals",
       x = "Standardized Residuals",
       y = "Count") +
  theme_minimal()

hist_residuals

#ggsave("HW2_Plots/OLS Residuals Histogram.png", plot = hist_residuals, width = 6, height = 4, units = "in", dpi = 300)
```

The graph above presents a histogram of the standardized residuals, which have a normal shape in accordance to one of the assumptions in OLS regression. 

### Standardized Residual by Predicted Value Scatter Plot


```{r}
#| echo: false
# Scatter plot.
predicted_residual_plot <- ggplot(predicted_residual_data, aes(x = R_PREDICTED, y = R_STANDARD)) + 
  geom_point(size = 2, alpha = 0.3, color = "midnightblue") +
  labs(title = "Predicted Values vs. Standardized Residuals", 
       x = "Predicted Values", 
       y = "Standardized Residuals") +
  theme_minimal()

predicted_residual_plot

#ggsave("HW2_Plots/OLS Residuals Scatter Plot.png", plot = predicted_residual_plot, width = 6, height = 4, units = "in", dpi = 300)
```

This presents a scatter plot of the standardized residual by the predicted value. Standardized residuals are residuals divided by the standard error, and they are used for comparing residuals of different observations to each other. Visually, the plot is homoscedastic, which is in accordance with one of the assumptions in OLS regression. Another assumption is that there are no vast outliers, and while it seems like there are a few outlier observations, they might not have significant leverage, except for perhaps the observation at the bottom that lies on approximately (11, -6) that deviates the generally constant varianceâ€”further analysis in removing outlier points and comparing with the original would be needed to determine if that is the case. 

### Choropleth Map of Standardized Regression Residuals

```{r}
#| include: false
shapefile$residuals_standardized <- predicted_residual_data$R_STANDARD
```

```{r}
#| echo: false
choropleth_residuals <- ggplot() +
  geom_sf(data = shapefile, aes(fill = residuals_standardized), color = "cornsilk2", linewidth = 0.25) +
  scale_fill_viridis_c(option = "viridis") +
  labs(title = "Choropleth of Standardized Regression Residuals",
       fill = "Standardized Residuals") +
  theme_void()

choropleth_residuals
ggsave("choropleth_standardized_residuals.png", plot = choropleth_residuals, width = 8.5, height = 11, dpi = 600)
```

Referencing the maps of the dependent variable and the predictors presented earlier, there seems to be spatial autocorrelation in the variables as it seems that the block group observations are not independent of each other. There are many high-high house value and highhigh percentages of individuals with at least a bachelorâ€™s degree in Center City and in the Wissahickon Valley Park area toward northwest Philadelphia on the east side of the Schuylkill River. 

The percentage of detached homes had high-high value clusters in the Wissahickon Valley Park area as well, in addition to the most northeastern part of the county, which is quite in contrast to the low-low values for individuals living in poverty in that same region. Lastly, the percentage of vacancies seemed to have high-high value clusters in North Philadelphia. 

Perhaps not as stark as the other maps in the choropleth section, but when observing this one, the most distinct portion of the choropleth map is in the Wissahickon Valley Park region and there are low-low value clusters in North Philadelphia as well as South Philadelphia, so this choropleth map still violates the assumption that the block groups are independent of one another. 


## Additional Models

### Stepwise Regression

```{r}
#| echo: false
step_model <- step(regress_analysis, direction = "both")

step_model$anova
```

### Cross-Validation


```{r results='hide'}
#| echo: false
# Model 1
fit1 <- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES, data = regress_data)
#summary(fit1)
anova(fit1)

cv1 <- CVlm(data = regress_data, form.lm = fit1, m = 5, plotit = FALSE)

# Extract MSE and compute RMSE
mse1 <- attr(cv1, "ms")
rmse1 <- sqrt(mse1)

rmse1
```

```{r}
#| echo: false
# Model 2: LNMEDHVAL ~ PCTVACANT + MEDHHINC
fit2 <- lm(LNMEDHVAL ~ PCTVACANT + MEDHHINC, data = regress_data)
#summary(fit2)
#anova(fit2)

cv2 <- CVlm(data = regress_data, form.lm = fit2, m = 5, plotit = FALSE)

mse2 <- attr(cv2, "ms")
rmse2 <- sqrt(mse2)

rmse2
```

------------------------------------------------------------------------

# 4. DISCUSSION AND LIMITATIONS

This paper conducted a multiple linear regression with ğ‘¦ the natural log of the median house value (LNMEDHVAL) as the dependent, or response, variable and the following as the independent, or predictor, variables: ğ›½1 percent vacancies (PCTVACANT), ğ›½2 percent of detached single-family homes (PCTSINGLES), ğ›½3 percent of residents with at least a bachelorâ€™s degree or more (PCTBACHMOR), and ğ›½4 the natural log of the number of residents living in poverty (LNNBELPOV100). Concerning variable limitations, almost all predictors were measured in proportions, whereas the original NBELPOV100 measured raw count of individuals living in poverty, which would be a difficult statistic to compare to other block groups; the aggregation of fifty individuals in one small block group versus fifty individuals in a much larger block group is difficult to compare, but that small block group might only have 5% of residents living in poverty versus 50% in a larger block group. 

Before determining the above variables to conduct multiple linear regression on, exploratory data analysis was performed to check for assumptions. First, the summary statistics were observed, namely, the mean and the standard deviation for all variables, where the standard deviations were almost as high or even higher than their associated meansâ€™ values, which indicated high fluctuations in the observations. Second, histograms were plotted for all variables in their raw form and their log-transformed form to observe for normal distributionâ€”this is where the variables were solidified for the regression model because while all of the variables were highly skewed right, only two log-transformations were maintained on median household value and on the number of individuals living in poverty. This is because when the other variables were logtransformed, they had very large frequencies at zero, which made them inappropriate for regression modeling. Third, choropleth maps were created for the final variables to visually observe for spatial autocorrelation and multicollinearity along with a correlation matrix, and it was determined that while the predictor mapsâ€™ spatial patterns seemed inverted in certain areas, that multicollinearity was negligible. However, the maps had several high-high and low-low clusters, and the presence of spatial autocorrelation violated the assumption that the observations and residuals are independent. Fourth, a scatter plot and histogram of the standardized residuals were made, the former of which had a normal shape and the latter of which was homoscedastic, but the scatter plots of the predictor variables and the response variable presented a clearly nonlinear shape. This violated the assumption that the response and given predictor variable relationship is linear, meaning that a linear regression model is not ideal to represent these variablesâ€™ relationships, and could be rectified by using polynomial terms to explain the scatter plotsâ€™ curvatures. 

Despite some assumption violations described, this model was strong. The F-Ratio was 840.9 with a p-value less than 0.001, which was significant, indicating at least one of the predictors had a significant relationship with the response variable, and this was further confirmed with the ttests being also having a very small p-value when testing the relationships between the individual predictors and the response. The adjusted ğ‘…2 of the model indicated that the independent variables explained 66.15% of the variance in the dependent variable. Then, when running stepwise regression, the results showed that the final model kept all predictors in as in the original model because it had the lowest AIC value at -3448.073, and comparing its RMSE to another model with just PCTVACANT and MEDHHINC as predictors, the original had the lowest value. While these indicate the model is strong, it could potentially be improved by adding crime rates, or spatial predictors like distances to parks or other nearby amenities. 

Considering the use of Ridge or LASSO regression, neither are ideal for this situation as theyâ€™re both regularization regressions that seek to reduce overfitting, prioritizing prediction by introducing a penalty term to stabilize a model (Murel & Kavlakoglu, n.d.). While both regressions shrink the models with their penalties, Ridge keeps all predictors because it reduces their coefficients to near-zero and LASSO can remove some predictors it deems irrelevant to the response because it reduces their coefficients to exactly zero, the latter regression thereby acting as a predictor selector and simplifying modelsâ€”this is because Ridgeâ€™s penalty is the squared sum of coefficients and LASSOâ€™s penalty is the absolute value of the sum of coefficients (Murel & Kavlakoglu, n.d.). Their use-cases differ in this regard; Ridgeâ€™s performance excels when there are many predictors with roughly equivalent coefficients, and LASSOâ€™s performance excels when more predictors have very insignificant or zero coefficients. In this case, it makes no sense to implement these regressions, the current model is stable as all predictors are statistically significant to the response, thereâ€™s no multicollinearity, no heteroscedasticity, and the crossvalidated out-sample RMSE 0.3664 is very close to the in-sample residual standard error 0.3665, so the model generalizes well already and isnâ€™t overfit. 

------------------------------------------------------------------------

# WORKS CITED

Bieretz, B., & Schilling, J. (2019, July). *Pay for success and blighted properties: Insights and opportunities for     funding vacant property reclamation and neighborhood stabilization*. 
  Urban Institute. 
  https://www.urban.org/sites/default/files/publication/100464/pfs_and_blighted_properties _0.pdf 

Murel, J., & Kavlakoglu, E. (n.d.). What is regularization? 
  https://www.ibm.com/think/topics/regularization 

Wang, H., Cheng, Z., Smyth, R., Sun, G., Li, J., & Wang, W. (2022). *University education, homeownership and housing    wealth. China Economic Review*, 71, 101742. 
  https://doi.org/10.1016/j.chieco.2021.101742 
